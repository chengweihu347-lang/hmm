"""
ç½®ä¿¡åº¦åˆ†æå¯è§†åŒ–è„šæœ¬
éœ€è¦å…ˆè¿è¡Œä¸»è„šæœ¬ç”Ÿæˆ confidence_analysis_full.csv
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆæ ¹æ®ç³»ç»Ÿè°ƒæ•´ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è¯»å–åˆ†æç»“æœ
try:
    df = pd.read_csv('confidence_analysis_full.csv')
    print(f"æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•")
except FileNotFoundError:
    print("é”™è¯¯: è¯·å…ˆè¿è¡Œä¸»è„šæœ¬ç”Ÿæˆ confidence_analysis_full.csv")
    exit()

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.figure(figsize=(20, 12))

# ===============================
# å›¾1: ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
# ===============================
plt.subplot(2, 3, 1)
plt.hist(df['confidence_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(df['confidence_score'].mean(), color='red', linestyle='--', 
            label=f'Mean: {df["confidence_score"].mean():.3f}')
plt.axvline(df['confidence_score'].median(), color='green', linestyle='--', 
            label=f'Median: {df["confidence_score"].median():.3f}')
plt.xlabel('Confidence Score')
plt.ylabel('Frequency')
plt.title('Confidence Score Distribution')
plt.legend()
plt.grid(True, alpha=0.3)

# ===============================
# å›¾2: æ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯”
# ===============================
plt.subplot(2, 3, 2)
correct_conf = df[df['is_correct']]['confidence_score']
incorrect_conf = df[~df['is_correct']]['confidence_score']

plt.boxplot([correct_conf, incorrect_conf], 
            labels=['Correct', 'Incorrect'],
            patch_artist=True,
            boxprops=dict(facecolor='lightgreen', alpha=0.7),
            medianprops=dict(color='red', linewidth=2))
plt.ylabel('Confidence Score')
plt.title('Confidence: Correct vs Incorrect Predictions')
plt.grid(True, alpha=0.3)

# æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
plt.text(0.5, 0.95, f'Correct: Î¼={correct_conf.mean():.3f}', 
         transform=plt.gca().transAxes, fontsize=9)
plt.text(0.5, 0.90, f'Incorrect: Î¼={incorrect_conf.mean():.3f}', 
         transform=plt.gca().transAxes, fontsize=9)

# ===============================
# å›¾3: å„ç±»åˆ«å‡†ç¡®ç‡
# ===============================
plt.subplot(2, 3, 3)
class_accuracy = df.groupby('true_label').apply(
    lambda x: (x['is_correct'].sum() / len(x)) * 100
).sort_values()

colors = ['red' if acc < 60 else 'orange' if acc < 80 else 'green' 
          for acc in class_accuracy.values]

class_accuracy.plot(kind='barh', color=colors, alpha=0.7)
plt.xlabel('Accuracy (%)')
plt.ylabel('Class')
plt.title('Per-Class Accuracy')
plt.axvline(80, color='green', linestyle='--', alpha=0.5, label='Good (>80%)')
plt.axvline(60, color='orange', linestyle='--', alpha=0.5, label='Fair (>60%)')
plt.legend()
plt.grid(True, alpha=0.3)

# ===============================
# å›¾4: å„ç±»åˆ«å¹³å‡ç½®ä¿¡åº¦
# ===============================
plt.subplot(2, 3, 4)
class_confidence = df.groupby('true_label')['confidence_score'].mean().sort_values()
class_confidence.plot(kind='barh', color='steelblue', alpha=0.7)
plt.xlabel('Average Confidence')
plt.ylabel('Class')
plt.title('Per-Class Average Confidence')
plt.grid(True, alpha=0.3)

# ===============================
# å›¾5: ç½®ä¿¡åº¦ vs å‡†ç¡®ç‡ï¼ˆæ•£ç‚¹å›¾ï¼‰
# ===============================
plt.subplot(2, 3, 5)
conf_bins = pd.cut(df['confidence_score'], bins=10)
accuracy_by_conf = df.groupby(conf_bins).apply(
    lambda x: (x['is_correct'].sum() / len(x)) * 100 if len(x) > 0 else 0
)
count_by_conf = df.groupby(conf_bins).size()

x_positions = [interval.mid for interval in accuracy_by_conf.index]
plt.scatter(x_positions, accuracy_by_conf.values, 
           s=count_by_conf.values * 10, alpha=0.6, color='purple')
plt.plot(x_positions, accuracy_by_conf.values, 'r--', alpha=0.5)
plt.xlabel('Confidence Score')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy vs Confidence (bubble size = sample count)')
plt.grid(True, alpha=0.3)

# ===============================
# å›¾6: Marginåˆ†å¸ƒ
# ===============================
plt.subplot(2, 3, 6)
plt.hist(df['margin'], bins=30, alpha=0.7, color='coral', edgecolor='black')
plt.axvline(df['margin'].mean(), color='red', linestyle='--', 
            label=f'Mean: {df["margin"].mean():.2f}')
plt.xlabel('Margin (best - 2nd best score)')
plt.ylabel('Frequency')
plt.title('Margin Distribution')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('confidence_analysis_visualization.png', dpi=300, bbox_inches='tight')
print("\nâœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ä¸º: confidence_analysis_visualization.png")

# ===============================
# é¢å¤–å›¾è¡¨: æ··æ·†çŸ©é˜µçƒ­å›¾
# ===============================
plt.figure(figsize=(10, 8))
from sklearn.metrics import confusion_matrix

labels = sorted(df['true_label'].unique())
cm = confusion_matrix(df['true_label'], df['predicted_label'], labels=labels)

# è®¡ç®—ç™¾åˆ†æ¯”
cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='YlOrRd', 
            xticklabels=labels, yticklabels=labels,
            cbar_kws={'label': 'Percentage (%)'})
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Percentage)')
plt.tight_layout()
plt.savefig('confusion_matrix_heatmap.png', dpi=300, bbox_inches='tight')
print("âœ… æ··æ·†çŸ©é˜µçƒ­å›¾å·²ä¿å­˜ä¸º: confusion_matrix_heatmap.png")

# ===============================
# ç±»åˆ«è¯¦ç»†åˆ†æå›¾
# ===============================
unique_labels = sorted(df['true_label'].unique())
n_labels = len(unique_labels)

fig, axes = plt.subplots(n_labels, 2, figsize=(15, 4*n_labels))
if n_labels == 1:
    axes = axes.reshape(1, -1)

for idx, label in enumerate(unique_labels):
    label_data = df[df['true_label'] == label]
    
    # å·¦å›¾: è¯¥ç±»åˆ«çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
    ax1 = axes[idx, 0]
    correct_data = label_data[label_data['is_correct']]['confidence_score']
    incorrect_data = label_data[~label_data['is_correct']]['confidence_score']
    
    ax1.hist([correct_data, incorrect_data], bins=20, alpha=0.7,
             label=['Correct', 'Incorrect'], color=['green', 'red'])
    ax1.set_xlabel('Confidence Score')
    ax1.set_ylabel('Count')
    ax1.set_title(f'Class: {label} - Confidence Distribution')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å³å›¾: è¯¥ç±»åˆ«è¢«è¯¯åˆ†ç±»ä¸ºå“ªäº›ç±»åˆ«
    ax2 = axes[idx, 1]
    confused = label_data[~label_data['is_correct']]['predicted_label'].value_counts()
    if len(confused) > 0:
        confused.plot(kind='barh', ax=ax2, color='orange', alpha=0.7)
        ax2.set_xlabel('Count')
        ax2.set_title(f'Class: {label} - Confused As')
        ax2.grid(True, alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'No misclassifications!', 
                ha='center', va='center', fontsize=14, color='green')
        ax2.set_title(f'Class: {label} - Confused As')

plt.tight_layout()
plt.savefig('per_class_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… å„ç±»åˆ«è¯¦ç»†åˆ†æå·²ä¿å­˜ä¸º: per_class_analysis.png")

# ===============================
# ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
# ===============================
report = []
report.append("=" * 70)
report.append("ç½®ä¿¡åº¦åˆ†ææŠ¥å‘Š")
report.append("=" * 70)
report.append("")

# æ•´ä½“ç»Ÿè®¡
report.append("ğŸ“Š æ•´ä½“ç»Ÿè®¡:")
report.append(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
report.append(f"  æ•´ä½“å‡†ç¡®ç‡: {(df['is_correct'].sum() / len(df)) * 100:.2f}%")
report.append(f"  å¹³å‡ç½®ä¿¡åº¦: {df['confidence_score'].mean():.3f}")
report.append(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {df['confidence_score'].std():.3f}")
report.append("")

# æŒ‰ç½®ä¿¡åº¦åŒºé—´ç»Ÿè®¡
report.append("ğŸ“ˆ æŒ‰ç½®ä¿¡åº¦åŒºé—´ç»Ÿè®¡:")
bins = [0, 0.3, 0.5, 0.7, 1.0]
labels_bin = ['Low (0-0.3)', 'Medium-Low (0.3-0.5)', 'Medium-High (0.5-0.7)', 'High (0.7-1.0)']
df['conf_bin'] = pd.cut(df['confidence_score'], bins=bins, labels=labels_bin)

for bin_label in labels_bin:
    bin_data = df[df['conf_bin'] == bin_label]
    if len(bin_data) > 0:
        acc = (bin_data['is_correct'].sum() / len(bin_data)) * 100
        report.append(f"  {bin_label}: {len(bin_data)} æ ·æœ¬, å‡†ç¡®ç‡ {acc:.1f}%")
report.append("")

# å„ç±»åˆ«è¡¨ç°
report.append("ğŸ¯ å„ç±»åˆ«è¡¨ç°:")
for label in sorted(df['true_label'].unique()):
    label_data = df[df['true_label'] == label]
    acc = (label_data['is_correct'].sum() / len(label_data)) * 100
    avg_conf = label_data['confidence_score'].mean()
    report.append(f"  {label}:")
    report.append(f"    æ ·æœ¬æ•°: {len(label_data)}")
    report.append(f"    å‡†ç¡®ç‡: {acc:.2f}%")
    report.append(f"    å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}")
report.append("")

# é—®é¢˜æ ·æœ¬
low_conf_count = len(df[df['confidence_score'] < 0.3])
high_conf_errors = len(df[(~df['is_correct']) & (df['confidence_score'] > 0.7)])

report.append("âš ï¸ éœ€è¦å…³æ³¨çš„æ ·æœ¬:")
report.append(f"  ä½ç½®ä¿¡åº¦æ ·æœ¬ (<0.3): {low_conf_count}")
report.append(f"  é«˜ç½®ä¿¡åº¦é”™è¯¯ (>0.7): {high_conf_errors}")
report.append("")

# ä¿å­˜æŠ¥å‘Š
with open('confidence_analysis_report.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(report))

print("âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜ä¸º: confidence_analysis_report.txt")
print("\n" + "=" * 70)
print("å¯è§†åŒ–åˆ†æå®Œæˆï¼")
print("=" * 70)
print("\nç”Ÿæˆçš„æ–‡ä»¶:")
print("  1. confidence_analysis_visualization.png - 6ä¸ªç»¼åˆåˆ†æå›¾")
print("  2. confusion_matrix_heatmap.png - æ··æ·†çŸ©é˜µçƒ­å›¾")
print("  3. per_class_analysis.png - å„ç±»åˆ«è¯¦ç»†åˆ†æ")
print("  4. confidence_analysis_report.txt - æ–‡æœ¬åˆ†ææŠ¥å‘Š")
print("\nå»ºè®®æŸ¥çœ‹é¡ºåº:")
print("  1. å…ˆçœ‹ç»¼åˆåˆ†æå›¾ï¼Œäº†è§£æ•´ä½“æƒ…å†µ")
print("  2. æŸ¥çœ‹æ··æ·†çŸ©é˜µï¼Œæ‰¾å‡ºå®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹")
print("  3. çœ‹å„ç±»åˆ«è¯¦ç»†åˆ†æï¼Œé’ˆå¯¹æ€§æ”¹è¿›")
print("=" * 70)