import os
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from hmmlearn.hmm import GMMHMM
import warnings

warnings.filterwarnings("ignore")

# ===============================
# å‚æ•°è®¾ç½®
# ===============================
DATA_FOLDER = os.path.join(os.path.dirname(__file__), "demobb")
LABEL_FILE = os.path.join(os.path.dirname(__file__), "labels.csv")

N_COMPONENTS = 3
N_MIXTURES = 3
MAX_ITER = 100
COV_TYPE = "diag"
RANDOM_STATE = 42
SCALE_FEATURES = True
NAN_STRATEGY = "interpolate"
MIN_VALID_FRAMES = 10

# ç½®ä¿¡åº¦é˜ˆå€¼è®¾ç½®
CONFIDENCE_THRESHOLD_LOW = 0.3  # ä½ç½®ä¿¡åº¦é˜ˆå€¼
CONFIDENCE_THRESHOLD_HIGH = 0.7  # é«˜ç½®ä¿¡åº¦é˜ˆå€¼

# é¢„æµ‹ç­–ç•¥é€‰æ‹©: 'standard', 'confidence_weighted', 'margin_based'
PREDICTION_STRATEGY = "standard"

np.random.seed(RANDOM_STATE)


# ===============================
# æ•°æ®åŠ è½½å‡½æ•°
# ===============================
def load_json_angles(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    frames = data["frames"]

    seq = []
    for f in frames:
        angles = [
            f["joints"]["shoulder_angle"],
            f["joints"]["elbow_angle"],
            f["joints"]["hip_angle"],
            f["joints"]["knee_angle"],
            f["joints"]["ankle_angle"],
        ]
        seq.append(angles)

    seq = np.array(seq, dtype=float)

    has_nan = np.isnan(seq).any()
    if has_nan:
        if NAN_STRATEGY == "interpolate":
            seq = interpolate_nan(seq)
        elif NAN_STRATEGY == "drop":
            seq = drop_nan_frames(seq)

    return seq


def interpolate_nan(seq):
    df = pd.DataFrame(seq)
    df = df.interpolate(method="linear", limit_direction="both", axis=0)
    df = df.fillna(df.mean())
    df = df.fillna(0)
    return df.values


def drop_nan_frames(seq):
    mask = ~np.isnan(seq).any(axis=1)
    return seq[mask]


def is_valid_sequence(seq):
    if len(seq) < MIN_VALID_FRAMES:
        return False
    if np.isnan(seq).all():
        return False
    return True


# ===============================
# ç½®ä¿¡åº¦è®¡ç®—å‡½æ•°
# ===============================
def calculate_confidence_metrics(log_likelihoods):
    """
    è®¡ç®—å¤šç§ç½®ä¿¡åº¦æŒ‡æ ‡w

    Args:
        log_likelihoods: dict, {label: log_likelihood}

    Returns:
        dict: åŒ…å«å¤šç§ç½®ä¿¡åº¦æŒ‡æ ‡
    """
    sorted_scores = sorted(log_likelihoods.items(), key=lambda x: x[1], reverse=True)
    best_label, best_score = sorted_scores[0]
    second_best_score = sorted_scores[1][1] if len(sorted_scores) > 1 else -np.inf

    # 1. æ ‡å‡†åŒ–æ¦‚ç‡ï¼ˆSoftmaxï¼‰
    scores = np.array(list(log_likelihoods.values()))
    # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå‡å»æœ€å¤§å€¼
    exp_scores = np.exp(scores - np.max(scores))
    probabilities = exp_scores / np.sum(exp_scores)
    max_probability = np.max(probabilities)

    # 2. Margin (æœ€é«˜åˆ†ä¸ç¬¬äºŒé«˜åˆ†çš„å·®è·)
    margin = best_score - second_best_score

    # 3. Entropy (ä¸ç¡®å®šæ€§åº¦é‡)
    # é¿å…log(0)
    probabilities_safe = probabilities + 1e-10
    entropy = -np.sum(probabilities * np.log(probabilities_safe))
    # å½’ä¸€åŒ–entropyåˆ°[0,1]
    max_entropy = np.log(len(probabilities))
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0

    # 4. ç»¼åˆç½®ä¿¡åº¦åˆ†æ•° (0-1ä¹‹é—´)
    # ç»“åˆprobabilityå’Œmargin
    confidence_score = max_probability * (1 - normalized_entropy)

    return {
        "predicted_label": best_label,
        "max_probability": max_probability,
        "margin": margin,
        "entropy": entropy,
        "normalized_entropy": normalized_entropy,
        "confidence_score": confidence_score,
        "all_probabilities": dict(zip(log_likelihoods.keys(), probabilities)),
        "raw_scores": log_likelihoods,
    }


def predict_with_strategy(seq, models, strategy="standard"):
    """
    æ ¹æ®ä¸åŒç­–ç•¥è¿›è¡Œé¢„æµ‹

    Args:
        seq: è¾“å…¥åºåˆ—
        models: è®­ç»ƒå¥½çš„HMMæ¨¡å‹å­—å…¸
        strategy: é¢„æµ‹ç­–ç•¥

    Returns:
        predicted_label, confidence_metrics
    """
    logL = {label: models[label].score(seq) for label in models}
    confidence_metrics = calculate_confidence_metrics(logL)

    if strategy == "standard":
        # æ ‡å‡†ç­–ç•¥ï¼šé€‰æ‹©æœ€é«˜åˆ†
        pred_label = confidence_metrics["predicted_label"]

    elif strategy == "confidence_weighted":
        # ç½®ä¿¡åº¦åŠ æƒç­–ç•¥ï¼šä½ç½®ä¿¡åº¦æ—¶è¿”å›"uncertain"
        if confidence_metrics["confidence_score"] < CONFIDENCE_THRESHOLD_LOW:
            pred_label = "UNCERTAIN"
        else:
            pred_label = confidence_metrics["predicted_label"]

    elif strategy == "margin_based":
        # åŸºäºMarginçš„ç­–ç•¥ï¼šmarginå¤ªå°æ—¶è®¤ä¸ºä¸ç¡®å®š
        if confidence_metrics["margin"] < 5.0:  # å¯è°ƒæ•´é˜ˆå€¼
            pred_label = "UNCERTAIN"
        else:
            pred_label = confidence_metrics["predicted_label"]

    else:
        pred_label = confidence_metrics["predicted_label"]

    return pred_label, confidence_metrics


# ===============================
# åŠ è½½æ•°æ®
# ===============================
if not os.path.exists(LABEL_FILE):
    raise FileNotFoundError(f"æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {LABEL_FILE}")

labels_df = pd.read_csv(LABEL_FILE)
print(f"Loaded labels.csv: {len(labels_df)} entries")

sequences, labels, video_ids = [], [], []
skipped_count = 0

for _, row in labels_df.iterrows():
    json_path = os.path.join(DATA_FOLDER, row["video_id"])
    if os.path.exists(json_path):
        try:
            seq = load_json_angles(json_path)
            if is_valid_sequence(seq):
                sequences.append(seq)
                labels.append(row["label"])
                video_ids.append(row["video_id"])
            else:
                skipped_count += 1
        except Exception as e:
            print(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶å¤±è´¥: {row['video_id']}, é”™è¯¯: {e}")
            skipped_count += 1
    else:
        skipped_count += 1

print(f"\nå·²æˆåŠŸåŠ è½½ {len(sequences)} ä¸ªæ ·æœ¬")
print(f"è·³è¿‡/å¤±è´¥: {skipped_count} ä¸ªæ ·æœ¬")

label_counts = pd.Series(labels).value_counts()
print(f"\nå„ç±»åˆ«æ ·æœ¬æ•°:\n{label_counts}")

# ===============================
# åˆ’åˆ†æ•°æ®é›†
# ===============================
X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
    sequences,
    labels,
    video_ids,
    test_size=0.3,
    random_state=RANDOM_STATE,
    stratify=labels,
)
print(f"\nè®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")

# ===============================
# ç‰¹å¾ç¼©æ”¾
# ===============================
if SCALE_FEATURES:
    scaler = StandardScaler()
    all_train_frames = np.vstack(X_train)
    scaler.fit(all_train_frames)
    X_train = [scaler.transform(x) for x in X_train]
    X_test = [scaler.transform(x) for x in X_test]
    print("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")

# ===============================
# è®­ç»ƒæ¨¡å‹
# ===============================
models = {}
unique_labels = sorted(set(y_train))

for label in unique_labels:
    label_seqs = [X_train[i] for i in range(len(X_train)) if y_train[i] == label]
    X_concat = np.vstack(label_seqs)
    lengths = [len(x) for x in label_seqs]

    print(f"\næ­£åœ¨è®­ç»ƒ HMM æ¨¡å‹: {label} ({len(label_seqs)} æ¡åºåˆ—)...")
    model = GMMHMM(
        n_components=N_COMPONENTS,
        n_mix=N_MIXTURES,
        covariance_type=COV_TYPE,
        n_iter=MAX_ITER,
        random_state=RANDOM_STATE,
        verbose=False,
    )
    model.fit(X_concat, lengths)
    models[label] = model
    print(f"âœ… å®Œæˆ {label} æ¨¡å‹è®­ç»ƒ")

# ===============================
# é¢„æµ‹ä¸ç½®ä¿¡åº¦åˆ†æ
# ===============================
print(f"\nä½¿ç”¨é¢„æµ‹ç­–ç•¥: {PREDICTION_STRATEGY}")
y_pred = []
confidence_results = []

for i, seq in enumerate(X_test):
    pred_label, conf_metrics = predict_with_strategy(seq, models, PREDICTION_STRATEGY)
    y_pred.append(pred_label)

    # ä¿å­˜è¯¦ç»†çš„ç½®ä¿¡åº¦ä¿¡æ¯
    confidence_results.append(
        {
            "video_id": ids_test[i],
            "true_label": y_test[i],
            "predicted_label": pred_label,
            "confidence_score": conf_metrics["confidence_score"],
            "max_probability": conf_metrics["max_probability"],
            "margin": conf_metrics["margin"],
            "entropy": conf_metrics["normalized_entropy"],
            "is_correct": pred_label == y_test[i],
            "all_probs": conf_metrics["all_probabilities"],
        }
    )

# åˆ›å»ºç½®ä¿¡åº¦DataFrame
confidence_df = pd.DataFrame(confidence_results)

# ===============================
# åŸºæœ¬è¯„ä¼°ç»“æœ
# ===============================
labels_sorted = sorted(unique_labels)

# å¤„ç†UNCERTAINæ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
if "UNCERTAIN" in y_pred:
    y_pred_filtered = [
        p if p != "UNCERTAIN" else y_test[i] for i, p in enumerate(y_pred)
    ]
    print(f"\nå‘ç° {y_pred.count('UNCERTAIN')} ä¸ªä½ç½®ä¿¡åº¦é¢„æµ‹")
else:
    y_pred_filtered = y_pred

cm = confusion_matrix(y_test, y_pred_filtered, labels=labels_sorted)
acc = accuracy_score(y_test, y_pred_filtered)

print("\n" + "=" * 70)
print("=== åŸºæœ¬åˆ†ç±»ç»“æœ ===")
print("=" * 70)
print(f"å‡†ç¡®ç‡: {acc:.3f}")
print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")
print(
    f"\nè¯¦ç»†æŠ¥å‘Š:\n{classification_report(y_test, y_pred_filtered, target_names=labels_sorted)}"
)

# ===============================
# ç½®ä¿¡åº¦ç»Ÿè®¡åˆ†æ
# ===============================
print("\n" + "=" * 70)
print("=== ç½®ä¿¡åº¦åˆ†æ ===")
print("=" * 70)

print(f"\næ•´ä½“ç½®ä¿¡åº¦ç»Ÿè®¡:")
print(f"  å¹³å‡ç½®ä¿¡åº¦: {confidence_df['confidence_score'].mean():.3f}")
print(f"  ç½®ä¿¡åº¦ä¸­ä½æ•°: {confidence_df['confidence_score'].median():.3f}")
print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {confidence_df['confidence_score'].std():.3f}")
print(f"  æœ€ä½ç½®ä¿¡åº¦: {confidence_df['confidence_score'].min():.3f}")
print(f"  æœ€é«˜ç½®ä¿¡åº¦: {confidence_df['confidence_score'].max():.3f}")

# æŒ‰ç½®ä¿¡åº¦åŒºé—´ç»Ÿè®¡
print(f"\næŒ‰ç½®ä¿¡åº¦åŒºé—´åˆ†å¸ƒ:")
conf_bins = [0, 0.3, 0.5, 0.7, 1.0]
conf_labels = ["å¾ˆä½(0-0.3)", "ä½(0.3-0.5)", "ä¸­(0.5-0.7)", "é«˜(0.7-1.0)"]
confidence_df["conf_bin"] = pd.cut(
    confidence_df["confidence_score"], bins=conf_bins, labels=conf_labels
)
print(confidence_df["conf_bin"].value_counts().sort_index())

# æ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯”
print(f"\næ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯”:")
correct_conf = confidence_df[confidence_df["is_correct"]]["confidence_score"].mean()
incorrect_conf = confidence_df[~confidence_df["is_correct"]]["confidence_score"].mean()
print(f"  æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {correct_conf:.3f}")
print(f"  é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {incorrect_conf:.3f}")
print(f"  å·®å¼‚: {correct_conf - incorrect_conf:.3f}")

# ===============================
# å„ç±»åˆ«è¡¨ç°åˆ†æ
# ===============================
print("\n" + "=" * 70)
print("=== å„ç±»åˆ«è¯¦ç»†è¡¨ç°åˆ†æ ===")
print("=" * 70)

for label in labels_sorted:
    label_data = confidence_df[confidence_df["true_label"] == label]

    print(f"\nğŸ“Š ç±»åˆ«: {label}")
    print(f"{'='*60}")

    # åŸºæœ¬ç»Ÿè®¡
    total = len(label_data)
    correct = label_data["is_correct"].sum()
    accuracy = correct / total if total > 0 else 0

    print(f"æ ·æœ¬æ•°: {total}")
    print(f"å‡†ç¡®ç‡: {accuracy:.3f} ({correct}/{total})")

    # ç½®ä¿¡åº¦ç»Ÿè®¡
    avg_conf = label_data["confidence_score"].mean()
    print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}")

    # æ­£ç¡®å’Œé”™è¯¯æ ·æœ¬çš„ç½®ä¿¡åº¦
    if correct > 0:
        correct_avg = label_data[label_data["is_correct"]]["confidence_score"].mean()
        print(f"  â”œâ”€ æ­£ç¡®é¢„æµ‹: {correct_avg:.3f}")
    if correct < total:
        incorrect_avg = label_data[~label_data["is_correct"]]["confidence_score"].mean()
        print(f"  â””â”€ é”™è¯¯é¢„æµ‹: {incorrect_avg:.3f}")

    # æ··æ·†æƒ…å†µ
    if correct < total:
        confused_with = label_data[~label_data["is_correct"]][
            "predicted_label"
        ].value_counts()
        print(f"ä¸»è¦æ··æ·†ä¸º:")
        for conf_label, count in confused_with.items():
            print(f"  â†’ {conf_label}: {count} æ¬¡")

# ===============================
# ä½ç½®ä¿¡åº¦æ ·æœ¬æ ‡æ³¨
# ===============================
print("\n" + "=" * 70)
print("=== ä½ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆéœ€äººå·¥å¤æ ¸ï¼‰===")
print("=" * 70)

low_conf_samples = confidence_df[
    confidence_df["confidence_score"] < CONFIDENCE_THRESHOLD_LOW
].sort_values("confidence_score")

print(
    f"\nå‘ç° {len(low_conf_samples)} ä¸ªä½ç½®ä¿¡åº¦æ ·æœ¬ (é˜ˆå€¼ < {CONFIDENCE_THRESHOLD_LOW}):"
)
print(f"\n{'è§†é¢‘ID':<30} {'çœŸå®':<10} {'é¢„æµ‹':<10} {'ç½®ä¿¡åº¦':<10} {'æ­£ç¡®?'}")
print("-" * 70)

for _, row in low_conf_samples.head(20).iterrows():  # æ˜¾ç¤ºå‰20ä¸ª
    check = "âœ“" if row["is_correct"] else "âœ—"
    print(
        f"{row['video_id']:<30} {row['true_label']:<10} {row['predicted_label']:<10} {row['confidence_score']:<10.3f} {check}"
    )

# ä¿å­˜ä½ç½®ä¿¡åº¦æ ·æœ¬åˆ°æ–‡ä»¶
low_conf_file = "low_confidence_samples.csv"
low_conf_samples.to_csv(low_conf_file, index=False)
print(f"\nğŸ’¾ ä½ç½®ä¿¡åº¦æ ·æœ¬å·²ä¿å­˜è‡³: {low_conf_file}")

# ===============================
# é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰
# ===============================
print("\n" + "=" * 70)
print("=== é«˜ç½®ä¿¡åº¦ä½†é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ï¼ˆé‡ç‚¹ï¼ï¼‰===")
print("=" * 70)

high_conf_errors = confidence_df[
    (~confidence_df["is_correct"])
    & (confidence_df["confidence_score"] > CONFIDENCE_THRESHOLD_HIGH)
].sort_values("confidence_score", ascending=False)

print(
    f"\nå‘ç° {len(high_conf_errors)} ä¸ªé«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬ (ç½®ä¿¡åº¦ > {CONFIDENCE_THRESHOLD_HIGH}):"
)
print("è¿™äº›æ ·æœ¬å¯èƒ½å­˜åœ¨æ ‡ç­¾é”™è¯¯æˆ–ç‰¹å¾æå–é—®é¢˜ï¼\n")

if len(high_conf_errors) > 0:
    print(f"{'è§†é¢‘ID':<30} {'çœŸå®':<10} {'é¢„æµ‹':<10} {'ç½®ä¿¡åº¦':<10}")
    print("-" * 70)
    for _, row in high_conf_errors.head(10).iterrows():
        print(
            f"{row['video_id']:<30} {row['true_label']:<10} {row['predicted_label']:<10} {row['confidence_score']:<10.3f}"
        )

    high_conf_errors.to_csv("high_confidence_errors.csv", index=False)
    print(f"\nğŸ’¾ é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬å·²ä¿å­˜è‡³: high_confidence_errors.csv")

# ===============================
# è¾¹ç•Œæ ·æœ¬åˆ†æ
# ===============================
print("\n" + "=" * 70)
print("=== è¾¹ç•Œæ ·æœ¬åˆ†æï¼ˆMarginå°çš„æ ·æœ¬ï¼‰===")
print("=" * 70)

low_margin_samples = confidence_df.sort_values("margin").head(15)

print("\nMarginæœ€å°çš„15ä¸ªæ ·æœ¬ï¼ˆæœ€éš¾åŒºåˆ†ï¼‰:")
print(f"{'è§†é¢‘ID':<30} {'çœŸå®':<10} {'é¢„æµ‹':<10} {'Margin':<10} {'æ­£ç¡®?'}")
print("-" * 70)

for _, row in low_margin_samples.iterrows():
    check = "âœ“" if row["is_correct"] else "âœ—"
    print(
        f"{row['video_id']:<30} {row['true_label']:<10} {row['predicted_label']:<10} {row['margin']:<10.2f} {check}"
    )

# ===============================
# ç»¼åˆæ”¹è¿›å»ºè®®
# ===============================
print("\n" + "=" * 70)
print("=== ğŸ’¡ æ”¹è¿›å»ºè®® ===")
print("=" * 70)

# 1. æ‰¾å‡ºè¡¨ç°æœ€å·®çš„ç±»åˆ«
worst_class = (
    confidence_df.groupby("true_label")
    .apply(lambda x: x["is_correct"].sum() / len(x))
    .sort_values()
    .index[0]
)

worst_acc = (
    confidence_df.groupby("true_label")
    .apply(lambda x: x["is_correct"].sum() / len(x))
    .sort_values()
    .values[0]
)

print(f"\n1. ä¼˜å…ˆæ”¹è¿›ç±»åˆ«: {worst_class} (å‡†ç¡®ç‡: {worst_acc:.3f})")
print(f"   å»ºè®®: å¢åŠ è¯¥ç±»åˆ«è®­ç»ƒæ ·æœ¬ï¼Œæ£€æŸ¥æ ‡æ³¨è´¨é‡")

# 2. æ•°æ®è´¨é‡é—®é¢˜
print(f"\n2. æ•°æ®è´¨é‡æ£€æŸ¥:")
print(f"   - å¤æ ¸ {len(high_conf_errors)} ä¸ªé«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬çš„æ ‡ç­¾")
print(f"   - æ£€æŸ¥ {len(low_conf_samples)} ä¸ªä½ç½®ä¿¡åº¦æ ·æœ¬çš„ç‰¹å¾è´¨é‡")

# 3. æ¨¡å‹è°ƒä¼˜æ–¹å‘
avg_margin = confidence_df["margin"].mean()
print(f"\n3. æ¨¡å‹è°ƒä¼˜å»ºè®®:")
print(f"   - å½“å‰å¹³å‡Margin: {avg_margin:.2f}")
if avg_margin < 10:
    print(f"   - Marginè¾ƒå°ï¼Œå»ºè®®å¢åŠ N_COMPONENTSæˆ–N_MIXTURES")
if incorrect_conf > 0.6:
    print(f"   - é”™è¯¯é¢„æµ‹ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
    print(f"   - å»ºè®®: å‡å°æ¨¡å‹å¤æ‚åº¦æˆ–å¢åŠ è®­ç»ƒæ•°æ®")

# ä¿å­˜å®Œæ•´åˆ†æç»“æœ
full_analysis_file = "confidence_analysis_full.csv"
confidence_df.to_csv(full_analysis_file, index=False)
print(f"\nğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜è‡³: {full_analysis_file}")

print("\n" + "=" * 70)
