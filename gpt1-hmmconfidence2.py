import os
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    f1_score,
)
from hmmlearn.hmm import GMMHMM
import warnings

warnings.filterwarnings("ignore")

# ===============================
# å‚æ•°è®¾ç½®
# ===============================
DATA_FOLDER = os.path.join(os.path.dirname(__file__), "demobb")
LABEL_FILE = os.path.join(os.path.dirname(__file__), "labels.csv")

N_COMPONENTS = 3
N_MIXTURES = 3
MAX_ITER = 100
COV_TYPE = "diag"
RANDOM_STATE = 42
SCALE_FEATURES = True
NAN_STRATEGY = "interpolate"
MIN_VALID_FRAMES = 10

# ç½®ä¿¡åº¦é˜ˆå€¼è®¾ç½®
CONFIDENCE_THRESHOLD_LOW = 0.3
CONFIDENCE_THRESHOLD_HIGH = 0.7

# é¢„æµ‹ç­–ç•¥é€‰æ‹©: 'standard', 'confidence_weighted', 'margin_based'
PREDICTION_STRATEGY = "standard"

# åºåˆ—é•¿åº¦å½’ä¸€åŒ–ï¼šTrue=ä½¿ç”¨å¹³å‡ä¼¼ç„¶ï¼ŒFalse=ä½¿ç”¨æ€»ä¼¼ç„¶
USE_NORMALIZED_LIKELIHOOD = True  # æ¨èTrueï¼Œè§£å†³é•¿åº¦ä¸ç­‰é—®é¢˜

# äº¤å‰éªŒè¯è®¾ç½®
USE_CROSS_VALIDATION = True  # æ˜¯å¦è¿›è¡Œäº¤å‰éªŒè¯
N_FOLDS = 5  # äº¤å‰éªŒè¯æŠ˜æ•°

# ===============================
# !!! æ–°å¢ï¼šç‰¹å¾æƒé‡è®¾ç½® !!!
# (0:shoulder, 1:elbow, 2:hip, 3:knee, 4:ankle)
# ===============================
ANKLE_FEATURE_INDEX = 4
ANKLE_WEIGHT = 1  # æƒé‡å€¼ (1.0 = ä¸å˜, 2.0 = 2å€æƒé‡) é«˜/ä½æ•ˆæœéƒ½å·®


np.random.seed(RANDOM_STATE)


# ===============================
# æ•°æ®åŠ è½½å‡½æ•°
# ===============================
def load_json_angles(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    frames = data["frames"]

    seq = []
    for f in frames:
        angles = [
            f["joints"]["shoulder_angle"],
            f["joints"]["elbow_angle"],
            f["joints"]["hip_angle"],
            f["joints"]["knee_angle"],
            f["joints"]["ankle_angle"],
        ]
        seq.append(angles)

    seq = np.array(seq, dtype=float)
    has_nan = np.isnan(seq).any()
    if has_nan:
        if NAN_STRATEGY == "interpolate":
            seq = interpolate_nan(seq)
        elif NAN_STRATEGY == "drop":
            seq = drop_nan_frames(seq)

    return seq


def interpolate_nan(seq):
    df = pd.DataFrame(seq)
    df = df.interpolate(method="linear", limit_direction="both", axis=0)
    df = df.fillna(df.mean())
    df = df.fillna(0)
    return df.values


def drop_nan_frames(seq):
    mask = ~np.isnan(seq).any(axis=1)
    return seq[mask]


def is_valid_sequence(seq):
    if len(seq) < MIN_VALID_FRAMES:
        return False
    if np.isnan(seq).all():
        return False
    return True


# ===============================
# !!! æ–°å¢ï¼šç‰¹å¾åŠ æƒè¾…åŠ©å‡½æ•° !!!
# ===============================
def apply_feature_weights(seq_list, feature_index, weight):
    """
    åœ¨æ ‡å‡†åŒ–åå¯¹ç‰¹å®šç‰¹å¾åº”ç”¨æƒé‡
    """
    if weight == 1.0:
        return seq_list

    weighted_seq_list = []
    for seq in seq_list:
        new_seq = seq.copy()
        # å¯¹æŒ‡å®šåˆ—ï¼ˆç‰¹å¾ï¼‰ä¹˜ä»¥æƒé‡
        new_seq[:, feature_index] *= weight
        weighted_seq_list.append(new_seq)
    return weighted_seq_list


# ===============================
# æ”¹è¿›çš„ä¼¼ç„¶è®¡ç®—ï¼ˆå¤„ç†åºåˆ—é•¿åº¦ä¸ç­‰ï¼‰
# ===============================
def compute_normalized_likelihood(model, seq, use_normalized=True):
    """
    è®¡ç®—å½’ä¸€åŒ–çš„ä¼¼ç„¶å€¼ï¼Œè§£å†³åºåˆ—é•¿åº¦ä¸ç­‰çš„é—®é¢˜
    """
    raw_log_likelihood = model.score(seq)

    if use_normalized:
        # ä½¿ç”¨å¹³å‡ä¼¼ç„¶ï¼ˆé™¤ä»¥åºåˆ—é•¿åº¦ï¼‰
        normalized_likelihood = raw_log_likelihood / len(seq)
        return normalized_likelihood
    else:
        # ä½¿ç”¨åŸå§‹æ€»ä¼¼ç„¶
        return raw_log_likelihood


# ===============================
# ç½®ä¿¡åº¦è®¡ç®—å‡½æ•°
# ===============================
def calculate_confidence_metrics(log_likelihoods):
    """
    è®¡ç®—å¤šç§ç½®ä¿¡åº¦æŒ‡æ ‡
    """
    sorted_scores = sorted(log_likelihoods.items(), key=lambda x: x[1], reverse=True)
    best_label, best_score = sorted_scores[0]
    second_best_score = sorted_scores[1][1] if len(sorted_scores) > 1 else -np.inf

    # 1. æ ‡å‡†åŒ–æ¦‚ç‡ï¼ˆSoftmaxï¼‰
    scores = np.array(list(log_likelihoods.values()))
    exp_scores = np.exp(scores - np.max(scores))
    probabilities = exp_scores / np.sum(exp_scores)
    max_probability = np.max(probabilities)

    # 2. Margin
    margin = best_score - second_best_score

    # 3. Entropy
    probabilities_safe = probabilities + 1e-10
    entropy = -np.sum(probabilities * np.log(probabilities_safe))
    max_entropy = np.log(len(probabilities))
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0

    # 4. ç»¼åˆç½®ä¿¡åº¦åˆ†æ•°
    confidence_score = max_probability * (1 - normalized_entropy)

    return {
        "predicted_label": best_label,
        "max_probability": max_probability,
        "margin": margin,
        "entropy": entropy,
        "normalized_entropy": normalized_entropy,
        "confidence_score": confidence_score,
        "all_probabilities": dict(zip(log_likelihoods.keys(), probabilities)),
        "raw_scores": log_likelihoods,
    }


def predict_with_strategy(seq, models, strategy="standard"):
    """
    æ ¹æ®ä¸åŒç­–ç•¥è¿›è¡Œé¢„æµ‹ï¼ˆä½¿ç”¨å½’ä¸€åŒ–ä¼¼ç„¶ï¼‰
    """
    # ä½¿ç”¨å½’ä¸€åŒ–ä¼¼ç„¶è®¡ç®—
    logL = {
        label: compute_normalized_likelihood(
            models[label], seq, USE_NORMALIZED_LIKELIHOOD
        )
        for label in models
    }
    confidence_metrics = calculate_confidence_metrics(logL)

    if strategy == "standard":
        pred_label = confidence_metrics["predicted_label"]
    elif strategy == "confidence_weighted":
        if confidence_metrics["confidence_score"] < CONFIDENCE_THRESHOLD_LOW:
            pred_label = "UNCERTAIN"
        else:
            pred_label = confidence_metrics["predicted_label"]
    elif strategy == "margin_based":
        if confidence_metrics["margin"] < 5.0:
            pred_label = "UNCERTAIN"
        else:
            pred_label = confidence_metrics["predicted_label"]
    else:
        pred_label = confidence_metrics["predicted_label"]

    return pred_label, confidence_metrics


# ===============================
# äº¤å‰éªŒè¯å‡½æ•°
# ===============================
def perform_cross_validation(sequences, labels, n_folds=5):
    """
    æ‰§è¡Œäº¤å‰éªŒè¯ï¼Œè®¡ç®—å¹³å‡å‡†ç¡®ç‡å’Œæ ‡å‡†å·®
    """
    print(f"\n{'='*70}")
    print(f"æ‰§è¡Œ {n_folds} æŠ˜äº¤å‰éªŒè¯...")
    print(f"{'='*70}")

    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)
    cv_accuracies = []
    cv_f1_scores = []
    fold_num = 1

    for train_idx, val_idx in skf.split(sequences, labels):
        print(f"\nç¬¬ {fold_num}/{n_folds} æŠ˜:")

        # åˆ’åˆ†æ•°æ®
        X_train_fold = [sequences[i] for i in train_idx]
        y_train_fold = [labels[i] for i in train_idx]
        X_val_fold = [sequences[i] for i in val_idx]
        y_val_fold = [labels[i] for i in val_idx]

        # ç‰¹å¾æ ‡å‡†åŒ–
        if SCALE_FEATURES:
            scaler = StandardScaler()
            all_train_frames = np.vstack(X_train_fold)
            scaler.fit(all_train_frames)
            X_train_fold = [scaler.transform(x) for x in X_train_fold]
            X_val_fold = [scaler.transform(x) for x in X_val_fold]

        # !!! ä¿®æ”¹ï¼šåœ¨CVä¸­åº”ç”¨ç‰¹å¾æƒé‡ !!!
        if ANKLE_WEIGHT != 1.0:
            print(f"  (CV Fold {fold_num}: æ­£åœ¨åº”ç”¨ ankle æƒé‡ {ANKLE_WEIGHT}x)")
            X_train_fold = apply_feature_weights(
                X_train_fold, ANKLE_FEATURE_INDEX, ANKLE_WEIGHT
            )
            X_val_fold = apply_feature_weights(
                X_val_fold, ANKLE_FEATURE_INDEX, ANKLE_WEIGHT
            )

        # è®­ç»ƒæ¨¡å‹
        models_fold = {}
        unique_labels = sorted(set(y_train_fold))

        for label in unique_labels:
            label_seqs = [
                X_train_fold[i]
                for i in range(len(X_train_fold))
                if y_train_fold[i] == label
            ]
            X_concat = np.vstack(label_seqs)
            lengths = [len(x) for x in label_seqs]

            model = GMMHMM(
                n_components=N_COMPONENTS,
                n_mix=N_MIXTURES,
                covariance_type=COV_TYPE,
                n_iter=MAX_ITER,
                random_state=RANDOM_STATE,
                verbose=False,
            )
            model.fit(X_concat, lengths)
            models_fold[label] = model

        # é¢„æµ‹
        y_pred_fold = []
        for seq in X_val_fold:
            pred_label, _ = predict_with_strategy(seq, models_fold, "standard")
            y_pred_fold.append(pred_label)

        # è®¡ç®—æŒ‡æ ‡
        fold_acc = accuracy_score(y_val_fold, y_pred_fold)
        fold_f1 = f1_score(y_val_fold, y_pred_fold, average="macro", zero_division=0)

        cv_accuracies.append(fold_acc)
        cv_f1_scores.append(fold_f1)

        print(f"  å‡†ç¡®ç‡: {fold_acc:.4f}, F1åˆ†æ•°: {fold_f1:.4f}")
        fold_num += 1

    return cv_accuracies, cv_f1_scores


# ===============================
# åŠ è½½æ•°æ®
# ===============================
if not os.path.exists(LABEL_FILE):
    raise FileNotFoundError(f"æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {LABEL_FILE}")

labels_df = pd.read_csv(LABEL_FILE)
print(f"Loaded labels.csv: {len(labels_df)} entries")

sequences, labels, video_ids = [], [], []
skipped_count = 0

for _, row in labels_df.iterrows():
    json_path = os.path.join(DATA_FOLDER, row["video_id"])
    if os.path.exists(json_path):
        try:
            seq = load_json_angles(json_path)
            if is_valid_sequence(seq):
                sequences.append(seq)
                labels.append(row["label"])
                video_ids.append(row["video_id"])
            else:
                skipped_count += 1
        except Exception as e:
            print(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶å¤±è´¥: {row['video_id']}, é”™è¯¯: {e}")
            skipped_count += 1
    else:
        skipped_count += 1

print(f"\nå·²æˆåŠŸåŠ è½½ {len(sequences)} ä¸ªæ ·æœ¬")
print(f"è·³è¿‡/å¤±è´¥: {skipped_count} ä¸ªæ ·æœ¬")

label_counts = pd.Series(labels).value_counts()
print(f"\nå„ç±»åˆ«æ ·æœ¬æ•°:\n{label_counts}")

# æ˜¾ç¤ºåºåˆ—é•¿åº¦ç»Ÿè®¡
seq_lengths = [len(seq) for seq in sequences]
print(f"\nåºåˆ—é•¿åº¦ç»Ÿè®¡:")
print(f"  æœ€çŸ­: {min(seq_lengths)} å¸§")
print(f"  æœ€é•¿: {max(seq_lengths)} å¸§")
print(f"  å¹³å‡: {np.mean(seq_lengths):.1f} å¸§")
print(f"  æ ‡å‡†å·®: {np.std(seq_lengths):.1f} å¸§")
print(f"\nä½¿ç”¨å½’ä¸€åŒ–ä¼¼ç„¶: {'æ˜¯' if USE_NORMALIZED_LIKELIHOOD else 'å¦'}")

# ===============================
# äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
# ===============================
if USE_CROSS_VALIDATION and len(sequences) >= 10:
    cv_accuracies, cv_f1_scores = perform_cross_validation(sequences, labels, N_FOLDS)

    print(f"\n{'='*70}")
    print("=== äº¤å‰éªŒè¯ç»“æœæ±‡æ€» ===")
    print(f"{'='*70}")
    print(f"\nå‡†ç¡®ç‡:")
    print(f"  å„æŠ˜: {[f'{acc:.4f}' for acc in cv_accuracies]}")
    print(f"  å¹³å‡: {np.mean(cv_accuracies):.4f} Â± {np.std(cv_accuracies):.4f}")
    print(f"  æœ€å°: {np.min(cv_accuracies):.4f}")
    print(f"  æœ€å¤§: {np.max(cv_accuracies):.4f}")

    print(f"\nF1åˆ†æ•°:")
    print(f"  å„æŠ˜: {[f'{f1:.4f}' for f1 in cv_f1_scores]}")
    print(f"  å¹³å‡: {np.mean(cv_f1_scores):.4f} Â± {np.std(cv_f1_scores):.4f}")
    print(f"  æœ€å°: {np.min(cv_f1_scores):.4f}")
    print(f"  æœ€å¤§: {np.max(cv_f1_scores):.4f}")

    # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
    cv_results = pd.DataFrame(
        {
            "fold": range(1, N_FOLDS + 1),
            "accuracy": cv_accuracies,
            "f1_score": cv_f1_scores,
        }
    )
    cv_results.to_csv("cross_validation_results.csv", index=False)
    print(f"\nğŸ’¾ äº¤å‰éªŒè¯ç»“æœå·²ä¿å­˜è‡³: cross_validation_results.csv")

# ===============================
# åˆ’åˆ†æ•°æ®é›†
# ===============================
X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
    sequences,
    labels,
    video_ids,
    test_size=0.3,
    random_state=RANDOM_STATE,
    stratify=labels,
)
print(f"\n{'='*70}")
print(f"è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")

# ===============================
# ç‰¹å¾ç¼©æ”¾
# ===============================
if SCALE_FEATURES:
    scaler = StandardScaler()
    all_train_frames = np.vstack(X_train)
    scaler.fit(all_train_frames)
    X_train = [scaler.transform(x) for x in X_train]
    X_test = [scaler.transform(x) for x in X_test]
    print("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")

# !!! ä¿®æ”¹ï¼šåº”ç”¨ç‰¹å¾æƒé‡åˆ° è®­ç»ƒé›†/æµ‹è¯•é›† !!!
if ANKLE_WEIGHT != 1.0:
    print(f"åº”ç”¨ ankle æƒé‡ {ANKLE_WEIGHT}x åˆ° è®­ç»ƒé›†/æµ‹è¯•é›†...")
    X_train = apply_feature_weights(X_train, ANKLE_FEATURE_INDEX, ANKLE_WEIGHT)
    X_test = apply_feature_weights(X_test, ANKLE_FEATURE_INDEX, ANKLE_WEIGHT)

# ===============================
# è®­ç»ƒæ¨¡å‹
# ===============================
models = {}
unique_labels = sorted(set(y_train))

for label in unique_labels:
    label_seqs = [X_train[i] for i in range(len(X_train)) if y_train[i] == label]
    X_concat = np.vstack(label_seqs)
    lengths = [len(x) for x in label_seqs]

    print(f"\næ­£åœ¨è®­ç»ƒ HMM æ¨¡å‹: {label} ({len(label_seqs)} æ¡åºåˆ—)...")
    model = GMMHMM(
        n_components=N_COMPONENTS,
        n_mix=N_MIXTURES,
        covariance_type=COV_TYPE,
        n_iter=MAX_ITER,
        random_state=RANDOM_STATE,
        verbose=False,
    )
    model.fit(X_concat, lengths)
    models[label] = model
    print(f"âœ… å®Œæˆ {label} æ¨¡å‹è®­ç»ƒ")

# ===============================
# é¢„æµ‹ä¸ç½®ä¿¡åº¦åˆ†æ
# ===============================
print(f"\nä½¿ç”¨é¢„æµ‹ç­–ç•¥: {PREDICTION_STRATEGY}")
y_pred = []
confidence_results = []

for i, seq in enumerate(X_test):
    pred_label, conf_metrics = predict_with_strategy(seq, models, PREDICTION_STRATEGY)
    y_pred.append(pred_label)

    confidence_results.append(
        {
            "video_id": ids_test[i],
            "true_label": y_test[i],
            "predicted_label": pred_label,
            "sequence_length": len(seq),
            "confidence_score": conf_metrics["confidence_score"],
            "max_probability": conf_metrics["max_probability"],
            "margin": conf_metrics["margin"],
            "entropy": conf_metrics["normalized_entropy"],
            "is_correct": pred_label == y_test[i],
            "all_probs": conf_metrics["all_probabilities"],
        }
    )

confidence_df = pd.DataFrame(confidence_results)

# ===============================
# åŸºæœ¬è¯„ä¼°ç»“æœ
# ===============================
labels_sorted = sorted(unique_labels)

if "UNCERTAIN" in y_pred:
    y_pred_filtered = [
        p if p != "UNCERTAIN" else y_test[i] for i, p in enumerate(y_pred)
    ]
    print(f"\nå‘ç° {y_pred.count('UNCERTAIN')} ä¸ªä½ç½®ä¿¡åº¦é¢„æµ‹")
else:
    y_pred_filtered = y_pred

cm = confusion_matrix(y_test, y_pred_filtered, labels=labels_sorted)
acc = accuracy_score(y_test, y_pred_filtered)

# è®¡ç®—å„ç±»åˆ«çš„F1åˆ†æ•°
f1_per_class = f1_score(
    y_test, y_pred_filtered, labels=labels_sorted, average=None, zero_division=0
)
f1_macro = f1_score(y_test, y_pred_filtered, average="macro", zero_division=0)
f1_weighted = f1_score(y_test, y_pred_filtered, average="weighted", zero_division=0)

print("\n" + "=" * 70)
print("=== åŸºæœ¬åˆ†ç±»ç»“æœ ===")
print("=" * 70)
print(f"å‡†ç¡®ç‡: {acc:.3f}")
print(f"Macro F1åˆ†æ•°: {f1_macro:.3f}")
print(f"Weighted F1åˆ†æ•°: {f1_weighted:.3f}")

print(f"\nå„ç±»åˆ«F1åˆ†æ•°:")
for label, f1 in zip(labels_sorted, f1_per_class):
    print(f"  {label}: {f1:.3f}")

print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")
print(
    f"\nè¯¦ç»†æŠ¥å‘Š:\n{classification_report(y_test, y_pred_filtered, target_names=labels_sorted)}"
)

# ===============================
# ç½®ä¿¡åº¦ç»Ÿè®¡åˆ†æ
# ===============================
print("\n" + "=" * 70)
print("=== ç½®ä¿¡åº¦åˆ†æ ===")
print("=" * 70)

print(f"\næ•´ä½“ç½®ä¿¡åº¦ç»Ÿè®¡:")
print(f"  å¹³å‡ç½®ä¿¡åº¦: {confidence_df['confidence_score'].mean():.3f}")
print(f"  ç½®ä¿¡åº¦ä¸­ä½æ•°: {confidence_df['confidence_score'].median():.3f}")
print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {confidence_df['confidence_score'].std():.3f}")
print(f"  æœ€ä½ç½®ä¿¡åº¦: {confidence_df['confidence_score'].min():.3f}")
print(f"  æœ€é«˜ç½®ä¿¡åº¦: {confidence_df['confidence_score'].max():.3f}")

# æŒ‰ç½®ä¿¡åº¦åŒºé—´ç»Ÿè®¡
print(f"\næŒ‰ç½®ä¿¡åº¦åŒºé—´åˆ†å¸ƒ:")
conf_bins = [0, 0.3, 0.5, 0.7, 1.0]
conf_labels = ["å¾ˆä½(0-0.3)", "ä½(0.3-0.5)", "ä¸­(0.5-0.7)", "é«˜(0.7-1.0)"]
confidence_df["conf_bin"] = pd.cut(
    confidence_df["confidence_score"], bins=conf_bins, labels=conf_labels
)
print(confidence_df["conf_bin"].value_counts().sort_index())

# æ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯”
print(f"\næ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯”:")
correct_conf = confidence_df[confidence_df["is_correct"]]["confidence_score"].mean()
incorrect_conf = confidence_df[~confidence_df["is_correct"]]["confidence_score"].mean()
print(f"  æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {correct_conf:.3f}")
print(f"  é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {incorrect_conf:.3f}")
print(f"  å·®å¼‚: {correct_conf - incorrect_conf:.3f}")

# ===============================
# å„ç±»åˆ«è¡¨ç°åˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰
# ===============================
print("\n" + "=" * 70)
print("=== å„ç±»åˆ«è¯¦ç»†è¡¨ç°åˆ†æ ===")
print("=" * 70)

class_performance = []

for idx, label in enumerate(labels_sorted):
    label_data = confidence_df[confidence_df["true_label"] == label]

    print(f"\nğŸ“Š ç±»åˆ«: {label}")
    print(f"{'='*60}")

    # åŸºæœ¬ç»Ÿè®¡
    total = len(label_data)
    correct = label_data["is_correct"].sum()
    accuracy = correct / total if total > 0 else 0
    f1 = f1_per_class[idx]

    print(f"æ ·æœ¬æ•°: {total}")
    print(f"å‡†ç¡®ç‡: {accuracy:.3f} ({correct}/{total})")
    print(f"F1åˆ†æ•°: {f1:.3f}")

    # åºåˆ—é•¿åº¦ç»Ÿè®¡
    avg_length = label_data["sequence_length"].mean()
    print(f"å¹³å‡åºåˆ—é•¿åº¦: {avg_length:.1f} å¸§")

    # ç½®ä¿¡åº¦ç»Ÿè®¡
    avg_conf = label_data["confidence_score"].mean()
    print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}")

    # æ­£ç¡®å’Œé”™è¯¯æ ·æœ¬çš„ç½®ä¿¡åº¦
    if correct > 0:
        correct_avg = label_data[label_data["is_correct"]]["confidence_score"].mean()
        print(f"  â”œâ”€ æ­£ç¡®é¢„æµ‹: {correct_avg:.3f}")
    if correct < total:
        incorrect_avg = label_data[~label_data["is_correct"]]["confidence_score"].mean()
        print(f"  â””â”€ é”™è¯¯é¢„æµ‹: {incorrect_avg:.3f}")

    # æ··æ·†æƒ…å†µ
    if correct < total:
        confused_with = label_data[~label_data["is_correct"]][
            "predicted_label"
        ].value_counts()
        print(f"ä¸»è¦æ··æ·†ä¸º:")
        for conf_label, count in confused_with.items():
            print(f"  â†’ {conf_label}: {count} æ¬¡")

    # ä¿å­˜ç±»åˆ«æ€§èƒ½æ•°æ®
    class_performance.append(
        {
            "class": label,
            "samples": total,
            "accuracy": accuracy,
            "f1_score": f1,
            "avg_confidence": avg_conf,
            "avg_length": avg_length,
        }
    )

# ä¿å­˜ç±»åˆ«æ€§èƒ½æ±‡æ€»
class_perf_df = pd.DataFrame(class_performance)
class_perf_df.to_csv("per_class_performance.csv", index=False)
print(f"\nğŸ’¾ å„ç±»åˆ«æ€§èƒ½æ±‡æ€»å·²ä¿å­˜è‡³: per_class_performance.csv")

# ===============================
# ä½ç½®ä¿¡åº¦æ ·æœ¬æ ‡æ³¨
# ===============================
print("\n" + "=" * 70)
print("=== ä½ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆéœ€äººå·¥å¤æ ¸ï¼‰===")
print("=" * 70)

low_conf_samples = confidence_df[
    confidence_df["confidence_score"] < CONFIDENCE_THRESHOLD_LOW
].sort_values("confidence_score")

print(
    f"\nå‘ç° {len(low_conf_samples)} ä¸ªä½ç½®ä¿¡åº¦æ ·æœ¬ (é˜ˆå€¼ < {CONFIDENCE_THRESHOLD_LOW}):"
)
print(
    f"\n{'è§†é¢‘ID':<30} {'çœŸå®':<10} {'é¢„æµ‹':<10} {'é•¿åº¦':<8} {'ç½®ä¿¡åº¦':<10} {'æ­£ç¡®?'}"
)
print("-" * 80)

for _, row in low_conf_samples.head(20).iterrows():
    check = "âœ“" if row["is_correct"] else "âœ—"
    print(
        f"{row['video_id']:<30} {row['true_label']:<10} {row['predicted_label']:<10} "
        f"{row['sequence_length']:<8} {row['confidence_score']:<10.3f} {check}"
    )

low_conf_file = "low_confidence_samples.csv"
low_conf_samples.to_csv(low_conf_file, index=False)
print(f"\nğŸ’¾ ä½ç½®ä¿¡åº¦æ ·æœ¬å·²ä¿å­˜è‡³: {low_conf_file}")

# ===============================
# é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬
# ===============================
print("\n" + "=" * 70)
print("=== é«˜ç½®ä¿¡åº¦ä½†é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ï¼ˆé‡ç‚¹ï¼ï¼‰===")
print("=" * 70)

high_conf_errors = confidence_df[
    (~confidence_df["is_correct"])
    & (confidence_df["confidence_score"] > CONFIDENCE_THRESHOLD_HIGH)
].sort_values("confidence_score", ascending=False)

print(
    f"\nå‘ç° {len(high_conf_errors)} ä¸ªé«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬ (ç½®ä¿¡åº¦ > {CONFIDENCE_THRESHOLD_HIGH}):"
)
print("è¿™äº›æ ·æœ¬å¯èƒ½å­˜åœ¨æ ‡ç­¾é”™è¯¯æˆ–ç‰¹å¾æå–é—®é¢˜ï¼\n")

if len(high_conf_errors) > 0:
    print(f"{'è§†é¢‘ID':<30} {'çœŸå®':<10} {'é¢„æµ‹':<10} {'é•¿åº¦':<8} {'ç½®ä¿¡åº¦':<10}")
    print("-" * 80)
    for _, row in high_conf_errors.head(10).iterrows():
        print(
            f"{row['video_id']:<30} {row['true_label']:<10} {row['predicted_label']:<10} "
            f"{row['sequence_length']:<8} {row['confidence_score']:<10.3f}"
        )

    high_conf_errors.to_csv("high_confidence_errors.csv", index=False)
    print(f"\nğŸ’¾ é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬å·²ä¿å­˜è‡³: high_confidence_errors.csv")

# ===============================
# ç»¼åˆæ”¹è¿›å»ºè®®
# ===============================
print("\n" + "=" * 70)
print("=== ğŸ’¡ æ”¹è¿›å»ºè®® ===")
print("=" * 70)

# 1. æ‰¾å‡ºè¡¨ç°æœ€å·®çš„ç±»åˆ«
worst_class_idx = np.argmin([perf["accuracy"] for perf in class_performance])
worst_class_info = class_performance[worst_class_idx]

print(f"\n1. ä¼˜å…ˆæ”¹è¿›ç±»åˆ«: {worst_class_info['class']}")
print(f"   å‡†ç¡®ç‡: {worst_class_info['accuracy']:.3f}")
print(f"   F1åˆ†æ•°: {worst_class_info['f1_score']:.3f}")
print(f"   å»ºè®®: å¢åŠ è¯¥ç±»åˆ«è®­ç»ƒæ ·æœ¬ï¼Œæ£€æŸ¥æ ‡æ³¨è´¨é‡")

# 2. æ•°æ®è´¨é‡é—®é¢˜
print(f"\n2. æ•°æ®è´¨é‡æ£€æŸ¥:")
print(f"   - å¤æ ¸ {len(high_conf_errors)} ä¸ªé«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬çš„æ ‡ç­¾")
print(f"   - æ£€æŸ¥ {len(low_conf_samples)} ä¸ªä½ç½®ä¿¡åº¦æ ·æœ¬çš„ç‰¹å¾è´¨é‡")

# 3. æ¨¡å‹è°ƒä¼˜æ–¹å‘
avg_margin = confidence_df["margin"].mean()
print(f"\n3. æ¨¡å‹è°ƒä¼˜å»ºè®®:")
print(f"   - å½“å‰å¹³å‡Margin: {avg_margin:.2f}")
if avg_margin < 10:
    print(f"   - Marginè¾ƒå°ï¼Œå»ºè®®å¢åŠ N_COMPONENTSæˆ–N_MIXTURES")
if incorrect_conf > 0.6:
    print(f"   - é”™è¯¯é¢„æµ‹ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
    print(f"   - å»ºè®®: å‡å°æ¨¡å‹å¤æ‚åº¦æˆ–å¢åŠ è®­ç»ƒæ•°æ®")

# 4. åºåˆ—é•¿åº¦å½’ä¸€åŒ–æ•ˆæœ
if USE_NORMALIZED_LIKELIHOOD:
    print(f"\n4. åºåˆ—é•¿åº¦å½’ä¸€åŒ–:")
    print(f"   - å·²å¯ç”¨å½’ä¸€åŒ–ä¼¼ç„¶ï¼ˆé™¤ä»¥åºåˆ—é•¿åº¦ï¼‰")
    # åˆ†æé•¿çŸ­åºåˆ—çš„é¢„æµ‹è¡¨ç°
    median_length = confidence_df["sequence_length"].median()
    short_seqs = confidence_df[confidence_df["sequence_length"] < median_length]
    long_seqs = confidence_df[confidence_df["sequence_length"] >= median_length]

    if len(short_seqs) > 0 and len(long_seqs) > 0:
        short_acc = short_seqs["is_correct"].mean()
        long_acc = long_seqs["is_correct"].mean()
        print(f"   - çŸ­åºåˆ—(<{median_length:.0f}å¸§)å‡†ç¡®ç‡: {short_acc:.3f}")
        print(f"   - é•¿åºåˆ—(â‰¥{median_length:.0f}å¸§)å‡†ç¡®ç‡: {long_acc:.3f}")
        print(f"   - å·®å¼‚: {abs(short_acc - long_acc):.3f}")

# ä¿å­˜å®Œæ•´åˆ†æç»“æœ
full_analysis_file = "confidence_analysis_full.csv"
confidence_df.to_csv(full_analysis_file, index=False)
print(f"\nğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜è‡³: {full_analysis_file}")

print("\n" + "=" * 70)
print("åˆ†æå®Œæˆï¼")
print("=" * 70)
