import os
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    f1_score,
)
from sklearn.neighbors import KNeighborsClassifier
from dtaidistance import dtw
import warnings

warnings.filterwarnings("ignore")

# ===============================
# å‚æ•°è®¾ç½®    äº¤å‰éªŒè¯å·²ç¦ç”¨ (PERFORM_CV=False)
# ===============================
DATA_FOLDER = os.path.join(os.path.dirname(__file__), "demobb")
LABEL_FILE = os.path.join(os.path.dirname(__file__), "labels.csv")

# DTW + KNN å‚æ•°
K_NEIGHBORS = 5  # KNNçš„Kå€¼
DTW_WINDOW = 50  # DTWçª—å£å¤§å°ï¼ˆè‡ªé€‚åº”è°ƒæ•´ï¼Œé˜²æ­¢æ— ç©·å¤§ï¼‰
DTW_PSI = 2  # DTWçš„psiå‚æ•°ï¼ˆç«¯ç‚¹æ¾å¼›ï¼Œé™ä½ä»¥æé«˜ç¨³å®šæ€§ï¼‰
USE_ADAPTIVE_WINDOW = True  # æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”çª—å£
MAX_DTW_DISTANCE = 1e6  # DTWæœ€å¤§è·ç¦»é˜ˆå€¼ï¼Œè¶…è¿‡è§†ä¸ºæ— ç©·å¤§

# äº¤å‰éªŒè¯å‚æ•°
N_FOLDS = 5  # KæŠ˜äº¤å‰éªŒè¯çš„æŠ˜æ•°
PERFORM_CV = False  # æ˜¯å¦æ‰§è¡Œäº¤å‰éªŒè¯ï¼ˆDTWè®¡ç®—æ…¢ï¼Œé»˜è®¤å…³é—­ï¼‰

RANDOM_STATE = 42
SCALE_FEATURES = True
NAN_STRATEGY = "interpolate"
MIN_VALID_FRAMES = 10
MAX_VALID_FRAMES = 500  # æœ€å¤§å¸§æ•°é™åˆ¶ï¼Œé˜²æ­¢åºåˆ—è¿‡é•¿
OUTLIER_CLIP_PERCENTILE = 99  # å¼‚å¸¸å€¼è£å‰ªç™¾åˆ†ä½

# ç‰¹å¾é…ç½®
USE_ANGLE = True  # æ˜¯å¦ä½¿ç”¨è§’åº¦
USE_VELOCITY = False  # æ˜¯å¦ä½¿ç”¨è§’é€Ÿåº¦
USE_ACCELERATION = False  # æ˜¯å¦ä½¿ç”¨è§’åŠ é€Ÿåº¦

# ç½®ä¿¡åº¦é˜ˆå€¼è®¾ç½®
CONFIDENCE_THRESHOLD_LOW = 0.3
CONFIDENCE_THRESHOLD_HIGH = 0.7

PREDICTION_STRATEGY = "standard"

np.random.seed(RANDOM_STATE)


# ===============================
# ç‰¹å¾æå–å‡½æ•°
# ===============================
def compute_derivatives(angles):
    """
    è®¡ç®—è§’åº¦çš„ä¸€é˜¶å¯¼æ•°(è§’é€Ÿåº¦)å’ŒäºŒé˜¶å¯¼æ•°(è§’åŠ é€Ÿåº¦)

    Args:
        angles: shape (n_frames, n_joints) çš„è§’åº¦åºåˆ—

    Returns:
        velocity: è§’é€Ÿåº¦
        acceleration: è§’åŠ é€Ÿåº¦
    """
    velocity = np.gradient(angles, axis=0)
    acceleration = np.gradient(velocity, axis=0)

    return velocity, acceleration


def extract_features(seq):
    """
    æ ¹æ®é…ç½®æå–ç‰¹å¾ï¼šè§’åº¦ã€è§’é€Ÿåº¦ã€è§’åŠ é€Ÿåº¦

    Args:
        seq: shape (n_frames, 5) çš„è§’åº¦åºåˆ—

    Returns:
        features: ç»„åˆåçš„ç‰¹å¾çŸ©é˜µ
    """
    features_list = []

    if USE_ANGLE:
        features_list.append(seq)

    if USE_VELOCITY:
        velocity, _ = compute_derivatives(seq)
        features_list.append(velocity)

    if USE_ACCELERATION:
        _, acceleration = compute_derivatives(seq)
        features_list.append(acceleration)

    if len(features_list) == 0:
        raise ValueError("è‡³å°‘éœ€è¦é€‰æ‹©ä¸€ç§ç‰¹å¾ç±»å‹ï¼")

    features = np.hstack(features_list)

    return features


# ===============================
# æ•°æ®åŠ è½½å‡½æ•°
# ===============================
def load_json_angles(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    frames = data["frames"]

    seq = []
    for f in frames:
        angles = [
            f["joints"]["shoulder_angle"],
            f["joints"]["elbow_angle"],
            f["joints"]["hip_angle"],
            f["joints"]["knee_angle"],
            f["joints"]["ankle_angle"],
        ]
        seq.append(angles)

    seq = np.array(seq, dtype=float)

    has_nan = np.isnan(seq).any()
    if has_nan:
        if NAN_STRATEGY == "interpolate":
            seq = interpolate_nan(seq)
        elif NAN_STRATEGY == "drop":
            seq = drop_nan_frames(seq)

    features = extract_features(seq)

    return features


def interpolate_nan(seq):
    df = pd.DataFrame(seq)
    df = df.interpolate(method="linear", limit_direction="both", axis=0)
    df = df.fillna(df.mean())
    df = df.fillna(0)
    return df.values


def drop_nan_frames(seq):
    mask = ~np.isnan(seq).any(axis=1)
    return seq[mask]


def is_valid_sequence(seq):
    """éªŒè¯åºåˆ—æ˜¯å¦æœ‰æ•ˆ"""
    if len(seq) < MIN_VALID_FRAMES:
        return False
    if len(seq) > MAX_VALID_FRAMES:
        return False
    if np.isnan(seq).all():
        return False
    if np.isinf(seq).any():  # æ£€æŸ¥æ— ç©·å¤§å€¼
        return False
    return True


def clip_outliers(seq, percentile=99):
    """
    è£å‰ªå¼‚å¸¸å€¼åˆ°æŒ‡å®šç™¾åˆ†ä½

    Args:
        seq: shape (n_frames, n_features)
        percentile: è£å‰ªç™¾åˆ†ä½æ•°

    Returns:
        clipped_seq: è£å‰ªåçš„åºåˆ—
    """
    seq_clipped = seq.copy()

    for i in range(seq.shape[1]):
        col = seq[:, i]
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        lower = np.percentile(col, 100 - percentile)
        upper = np.percentile(col, percentile)
        # è£å‰ª
        seq_clipped[:, i] = np.clip(col, lower, upper)

    return seq_clipped


def resample_sequence(seq, max_length=None):
    """
    å¦‚æœåºåˆ—è¿‡é•¿ï¼Œè¿›è¡Œé‡é‡‡æ ·

    Args:
        seq: shape (n_frames, n_features)
        max_length: æœ€å¤§é•¿åº¦

    Returns:
        resampled_seq: é‡é‡‡æ ·åçš„åºåˆ—
    """
    if max_length is None or len(seq) <= max_length:
        return seq

    # ä½¿ç”¨çº¿æ€§æ’å€¼é‡é‡‡æ ·
    original_indices = np.linspace(0, len(seq) - 1, len(seq))
    target_indices = np.linspace(0, len(seq) - 1, max_length)

    resampled = np.zeros((max_length, seq.shape[1]))
    for i in range(seq.shape[1]):
        resampled[:, i] = np.interp(target_indices, original_indices, seq[:, i])

    return resampled


# ===============================
# DTWè·ç¦»è®¡ç®—å‡½æ•°
# ===============================
def compute_dtw_distance(seq1, seq2):
    """
    è®¡ç®—ä¸¤ä¸ªæ—¶é—´åºåˆ—ä¹‹é—´çš„DTWè·ç¦»ï¼ˆå¸¦å¼‚å¸¸å¤„ç†ï¼‰

    Args:
        seq1: shape (n_frames1, n_features)
        seq2: shape (n_frames2, n_features)

    Returns:
        distance: DTWè·ç¦»
    """
    # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
    if np.isnan(seq1).any() or np.isnan(seq2).any():
        return MAX_DTW_DISTANCE

    if np.isinf(seq1).any() or np.isinf(seq2).any():
        return MAX_DTW_DISTANCE

    # è®¡ç®—è‡ªé€‚åº”çª—å£å¤§å°
    if USE_ADAPTIVE_WINDOW:
        # çª—å£å¤§å°ä¸ºä¸¤ä¸ªåºåˆ—é•¿åº¦å·®çš„ç»å¯¹å€¼ + ä¸€ä¸ªç¼“å†²
        len_diff = abs(len(seq1) - len(seq2))
        adaptive_window = min(DTW_WINDOW, max(10, len_diff + 10))
    else:
        adaptive_window = DTW_WINDOW

    total_distance = 0.0
    n_features = seq1.shape[1]
    valid_features = 0

    for i in range(n_features):
        try:
            s1 = seq1[:, i].astype(np.float64)
            s2 = seq2[:, i].astype(np.float64)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«NaNæˆ–Inf
            if np.isnan(s1).any() or np.isnan(s2).any():
                continue
            if np.isinf(s1).any() or np.isinf(s2).any():
                continue

            # ä½¿ç”¨dtaidistanceåº“è®¡ç®—DTWè·ç¦»
            distance = dtw.distance(s1, s2, window=adaptive_window, psi=DTW_PSI)

            # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if np.isnan(distance) or np.isinf(distance):
                distance = MAX_DTW_DISTANCE / n_features

            # é™åˆ¶å•ä¸ªç‰¹å¾çš„æœ€å¤§è·ç¦»
            distance = min(distance, MAX_DTW_DISTANCE / n_features)

            total_distance += distance
            valid_features += 1

        except Exception as e:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨æœ€å¤§è·ç¦»
            print(f"    è­¦å‘Š: ç‰¹å¾{i}çš„DTWè®¡ç®—å¤±è´¥: {e}")
            total_distance += MAX_DTW_DISTANCE / n_features
            valid_features += 1

    # è¿”å›å¹³å‡è·ç¦»
    if valid_features == 0:
        return MAX_DTW_DISTANCE

    avg_distance = total_distance / valid_features

    # æœ€ç»ˆæ£€æŸ¥
    if np.isnan(avg_distance) or np.isinf(avg_distance):
        return MAX_DTW_DISTANCE

    return avg_distance


# ===============================
# è‡ªå®šä¹‰DTW-KNNåˆ†ç±»å™¨
# ===============================
class DTW_KNN_Classifier:
    """
    åŸºäºDTWè·ç¦»çš„KNNåˆ†ç±»å™¨
    """

    def __init__(self, n_neighbors=5):
        self.n_neighbors = n_neighbors
        self.X_train = None
        self.y_train = None

    def fit(self, X_train, y_train):
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆå®é™…ä¸Šåªæ˜¯å­˜å‚¨è®­ç»ƒæ•°æ®ï¼‰

        Args:
            X_train: list of arrays, è®­ç»ƒåºåˆ—
            y_train: list, è®­ç»ƒæ ‡ç­¾
        """
        self.X_train = X_train
        self.y_train = np.array(y_train)
        return self

    def predict_single(self, seq):
        """
        å¯¹å•ä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹

        Args:
            seq: shape (n_frames, n_features)

        Returns:
            predicted_label: é¢„æµ‹æ ‡ç­¾
            distances: Kä¸ªæœ€è¿‘é‚»çš„è·ç¦»
            neighbors_labels: Kä¸ªæœ€è¿‘é‚»çš„æ ‡ç­¾
        """
        # è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„DTWè·ç¦»
        distances = []
        for train_seq in self.X_train:
            dist = compute_dtw_distance(seq, train_seq)
            distances.append(dist)

        distances = np.array(distances)

        # æ‰¾åˆ°Kä¸ªæœ€è¿‘é‚»
        k_nearest_indices = np.argsort(distances)[: self.n_neighbors]
        k_nearest_distances = distances[k_nearest_indices]
        k_nearest_labels = self.y_train[k_nearest_indices]

        # æŠ•ç¥¨å†³å®šé¢„æµ‹æ ‡ç­¾
        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)
        predicted_label = unique_labels[np.argmax(counts)]

        return predicted_label, k_nearest_distances, k_nearest_labels

    def predict(self, X_test):
        """
        å¯¹å¤šä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹

        Args:
            X_test: list of arrays

        Returns:
            predictions: list of predicted labels
        """
        predictions = []
        for seq in X_test:
            pred_label, _, _ = self.predict_single(seq)
            predictions.append(pred_label)
        return predictions


# ===============================
# ç½®ä¿¡åº¦è®¡ç®—å‡½æ•°ï¼ˆåŸºäºDTWè·ç¦»ï¼‰
# ===============================
def calculate_confidence_metrics_dtw(distances, neighbors_labels, all_labels):
    """
    åŸºäºDTWè·ç¦»å’Œæœ€è¿‘é‚»æ ‡ç­¾è®¡ç®—ç½®ä¿¡åº¦

    Args:
        distances: Kä¸ªæœ€è¿‘é‚»çš„è·ç¦»
        neighbors_labels: Kä¸ªæœ€è¿‘é‚»çš„æ ‡ç­¾
        all_labels: æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾

    Returns:
        dict: åŒ…å«ç½®ä¿¡åº¦æŒ‡æ ‡
    """
    # 1. åŸºäºæŠ•ç¥¨çš„æ¦‚ç‡ä¼°è®¡
    unique_labels, counts = np.unique(neighbors_labels, return_counts=True)
    vote_probs = counts / len(neighbors_labels)

    # æ„å»ºæ‰€æœ‰æ ‡ç­¾çš„æ¦‚ç‡å­—å…¸
    label_probs = {label: 0.0 for label in all_labels}
    for label, prob in zip(unique_labels, vote_probs):
        label_probs[label] = prob

    predicted_label = unique_labels[np.argmax(counts)]
    max_probability = np.max(vote_probs)

    # 2. åŸºäºè·ç¦»çš„æƒé‡æ¦‚ç‡ï¼ˆè·ç¦»è¶Šå°ï¼Œæƒé‡è¶Šå¤§ï¼‰
    # ä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°
    weights = np.exp(-distances / np.mean(distances))
    weights = weights / np.sum(weights)

    weighted_probs = {}
    for label in all_labels:
        mask = neighbors_labels == label
        weighted_probs[label] = np.sum(weights[mask])

    # 3. Margin (æœ€é«˜æ¦‚ç‡ä¸ç¬¬äºŒé«˜æ¦‚ç‡çš„å·®è·)
    sorted_probs = sorted(vote_probs, reverse=True)
    margin = sorted_probs[0] - (sorted_probs[1] if len(sorted_probs) > 1 else 0)

    # 4. Entropy
    probs = np.array(list(label_probs.values()))
    probs_safe = probs + 1e-10
    entropy = -np.sum(probs * np.log(probs_safe))
    max_entropy = np.log(len(all_labels))
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0

    # 5. ç»¼åˆç½®ä¿¡åº¦åˆ†æ•°
    # è€ƒè™‘æŠ•ç¥¨ä¸€è‡´æ€§å’Œè·ç¦»
    avg_distance = np.mean(distances)
    distance_confidence = 1.0 / (1.0 + avg_distance / 100)  # å½’ä¸€åŒ–è·ç¦»
    confidence_score = max_probability * distance_confidence * (1 - normalized_entropy)

    return {
        "predicted_label": predicted_label,
        "max_probability": max_probability,
        "margin": margin,
        "entropy": entropy,
        "normalized_entropy": normalized_entropy,
        "confidence_score": confidence_score,
        "all_probabilities": label_probs,
        "weighted_probabilities": weighted_probs,
        "avg_distance": avg_distance,
        "min_distance": np.min(distances),
    }


def predict_with_strategy(seq, classifier, all_labels, strategy="standard"):
    """
    æ ¹æ®ä¸åŒç­–ç•¥è¿›è¡Œé¢„æµ‹

    Args:
        seq: è¾“å…¥åºåˆ—
        classifier: DTW-KNNåˆ†ç±»å™¨
        all_labels: æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾åˆ—è¡¨
        strategy: é¢„æµ‹ç­–ç•¥

    Returns:
        predicted_label, confidence_metrics
    """
    pred_label, distances, neighbors_labels = classifier.predict_single(seq)
    confidence_metrics = calculate_confidence_metrics_dtw(
        distances, neighbors_labels, all_labels
    )

    if strategy == "standard":
        final_pred = confidence_metrics["predicted_label"]
    elif strategy == "confidence_weighted":
        if confidence_metrics["confidence_score"] < CONFIDENCE_THRESHOLD_LOW:
            final_pred = "UNCERTAIN"
        else:
            final_pred = confidence_metrics["predicted_label"]
    elif strategy == "margin_based":
        if confidence_metrics["margin"] < 0.3:  # è°ƒæ•´é˜ˆå€¼
            final_pred = "UNCERTAIN"
        else:
            final_pred = confidence_metrics["predicted_label"]
    else:
        final_pred = confidence_metrics["predicted_label"]

    return final_pred, confidence_metrics


# ===============================
# åŠ è½½æ•°æ®
# ===============================
print("=" * 70)
print("æ¨¡å‹é…ç½®: DTW + KNN")
print(f"  - Kå€¼: {K_NEIGHBORS}")
print(f"  - DTWçª—å£: {DTW_WINDOW if DTW_WINDOW else 'æ— é™åˆ¶'}")
print("=" * 70)
print("ç‰¹å¾é…ç½®:")
print(f"  - ä½¿ç”¨è§’åº¦: {USE_ANGLE}")
print(f"  - ä½¿ç”¨è§’é€Ÿåº¦: {USE_VELOCITY}")
print(f"  - ä½¿ç”¨è§’åŠ é€Ÿåº¦: {USE_ACCELERATION}")

# è®¡ç®—ç‰¹å¾ç»´åº¦
feature_dim = 0
if USE_ANGLE:
    feature_dim += 5
if USE_VELOCITY:
    feature_dim += 5
if USE_ACCELERATION:
    feature_dim += 5
print(f"  - æ€»ç‰¹å¾ç»´åº¦: {feature_dim}")
print("=" * 70)

if not os.path.exists(LABEL_FILE):
    raise FileNotFoundError(f"æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {LABEL_FILE}")

labels_df = pd.read_csv(LABEL_FILE)
print(f"\nLoaded labels.csv: {len(labels_df)} entries")

sequences, labels, video_ids = [], [], []
skipped_count = 0

for _, row in labels_df.iterrows():
    json_path = os.path.join(DATA_FOLDER, row["video_id"])
    if os.path.exists(json_path):
        try:
            seq = load_json_angles(json_path)
            if is_valid_sequence(seq):
                sequences.append(seq)
                labels.append(row["label"])
                video_ids.append(row["video_id"])
            else:
                skipped_count += 1
        except Exception as e:
            print(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶å¤±è´¥: {row['video_id']}, é”™è¯¯: {e}")
            skipped_count += 1
    else:
        skipped_count += 1

print(f"\nå·²æˆåŠŸåŠ è½½ {len(sequences)} ä¸ªæ ·æœ¬")
print(f"è·³è¿‡/å¤±è´¥: {skipped_count} ä¸ªæ ·æœ¬")

label_counts = pd.Series(labels).value_counts()
print(f"\nå„ç±»åˆ«æ ·æœ¬æ•°:\n{label_counts}")

print("\n" + "=" * 70)
print("=== äº¤å‰éªŒè¯è¯„ä¼° ===")
print("=" * 70)

if PERFORM_CV and len(sequences) >= N_FOLDS:
    print(f"\næ­£åœ¨è¿›è¡Œ {N_FOLDS} æŠ˜äº¤å‰éªŒè¯...")
    print("âš ï¸  è­¦å‘Š: DTWè®¡ç®—è¾ƒæ…¢ï¼Œäº¤å‰éªŒè¯å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")

    # ä½¿ç”¨StratifiedKFoldç¡®ä¿æ¯æŠ˜ä¸­ç±»åˆ«åˆ†å¸ƒä¸€è‡´
    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)

    cv_accuracies = []
    cv_f1_scores = []
    fold_idx = 1

    for train_idx, val_idx in skf.split(sequences, labels):
        print(f"\n  æŠ˜ {fold_idx}/{N_FOLDS}:")

        # åˆ’åˆ†æ•°æ®
        X_cv_train = [sequences[i] for i in train_idx]
        y_cv_train = [labels[i] for i in train_idx]
        X_cv_val = [sequences[i] for i in val_idx]
        y_cv_val = [labels[i] for i in val_idx]

        # ç‰¹å¾ç¼©æ”¾
        if SCALE_FEATURES:
            scaler_cv = StandardScaler()
            all_frames_cv = np.vstack(X_cv_train)
            scaler_cv.fit(all_frames_cv)
            X_cv_train = [scaler_cv.transform(x) for x in X_cv_train]
            X_cv_val = [scaler_cv.transform(x) for x in X_cv_val]

        # è®­ç»ƒå’Œé¢„æµ‹
        clf_cv = DTW_KNN_Classifier(n_neighbors=K_NEIGHBORS)
        clf_cv.fit(X_cv_train, y_cv_train)
        y_cv_pred = clf_cv.predict(X_cv_val)

        # è®¡ç®—æŒ‡æ ‡
        fold_acc = accuracy_score(y_cv_val, y_cv_pred)
        fold_f1 = f1_score(y_cv_val, y_cv_pred, average="macro")

        cv_accuracies.append(fold_acc)
        cv_f1_scores.append(fold_f1)

        print(f"    å‡†ç¡®ç‡: {fold_acc:.3f}")
        print(f"    å®å¹³å‡F1: {fold_f1:.3f}")

        fold_idx += 1

    # è®¡ç®—ç»Ÿè®¡é‡
    mean_acc = np.mean(cv_accuracies)
    std_acc = np.std(cv_accuracies)
    mean_f1 = np.mean(cv_f1_scores)
    std_f1 = np.std(cv_f1_scores)

    print("\n" + "-" * 70)
    print("äº¤å‰éªŒè¯ç»“æœæ±‡æ€»:")
    print("-" * 70)
    print(f"å¹³å‡å‡†ç¡®ç‡: {mean_acc:.3f} Â± {std_acc:.3f}")
    print(f"å‡†ç¡®ç‡èŒƒå›´: [{min(cv_accuracies):.3f}, {max(cv_accuracies):.3f}]")
    print(f"\nå¹³å‡å®F1åˆ†æ•°: {mean_f1:.3f} Â± {std_f1:.3f}")
    print(f"F1åˆ†æ•°èŒƒå›´: [{min(cv_f1_scores):.3f}, {max(cv_f1_scores):.3f}]")
    print("-" * 70)

    # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
    cv_results = pd.DataFrame(
        {
            "fold": range(1, N_FOLDS + 1),
            "accuracy": cv_accuracies,
            "macro_f1": cv_f1_scores,
        }
    )
    cv_results.to_csv("cross_validation_results.csv", index=False)
    print(f"\nğŸ’¾ äº¤å‰éªŒè¯ç»“æœå·²ä¿å­˜è‡³: cross_validation_results.csv")

else:
    if not PERFORM_CV:
        print("\nâš ï¸  äº¤å‰éªŒè¯å·²ç¦ç”¨ (PERFORM_CV=False)")
    else:
        print(f"\nâš ï¸  æ ·æœ¬æ•°({len(sequences)})å°‘äºæŠ˜æ•°({N_FOLDS})ï¼Œè·³è¿‡äº¤å‰éªŒè¯")

# ===============================
# åˆ’åˆ†æ•°æ®é›†
# ===============================
X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
    sequences,
    labels,
    video_ids,
    test_size=0.05,
    random_state=RANDOM_STATE,
    stratify=labels,
)
print(f"\nè®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")

# ===============================
# ç‰¹å¾ç¼©æ”¾
# ===============================
if SCALE_FEATURES:
    scaler = StandardScaler()
    all_train_frames = np.vstack(X_train)
    scaler.fit(all_train_frames)
    X_train = [scaler.transform(x) for x in X_train]
    X_test = [scaler.transform(x) for x in X_test]
    print("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")

# ===============================
# è®­ç»ƒDTW-KNNæ¨¡å‹
# ===============================
print("\næ­£åœ¨è®­ç»ƒ DTW-KNN æ¨¡å‹...")
classifier = DTW_KNN_Classifier(n_neighbors=K_NEIGHBORS)
classifier.fit(X_train, y_train)
print(f"âœ… DTW-KNN æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆå­˜å‚¨äº† {len(X_train)} ä¸ªè®­ç»ƒæ ·æœ¬ï¼‰")

# ===============================
# é¢„æµ‹ä¸ç½®ä¿¡åº¦åˆ†æ
# ===============================
print(f"\nä½¿ç”¨é¢„æµ‹ç­–ç•¥: {PREDICTION_STRATEGY}")
print("å¼€å§‹é¢„æµ‹ï¼ˆè®¡ç®—DTWè·ç¦»å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")

y_pred = []
confidence_results = []
unique_labels = sorted(set(y_train))

for i, seq in enumerate(X_test):
    if (i + 1) % 10 == 0:
        print(f"  è¿›åº¦: {i+1}/{len(X_test)}")

    pred_label, conf_metrics = predict_with_strategy(
        seq, classifier, unique_labels, PREDICTION_STRATEGY
    )
    y_pred.append(pred_label)

    confidence_results.append(
        {
            "video_id": ids_test[i],
            "true_label": y_test[i],
            "predicted_label": pred_label,
            "confidence_score": conf_metrics["confidence_score"],
            "max_probability": conf_metrics["max_probability"],
            "margin": conf_metrics["margin"],
            "entropy": conf_metrics["normalized_entropy"],
            "avg_distance": conf_metrics["avg_distance"],
            "min_distance": conf_metrics["min_distance"],
            "is_correct": pred_label == y_test[i],
            "all_probs": conf_metrics["all_probabilities"],
        }
    )

print("âœ… é¢„æµ‹å®Œæˆï¼")

confidence_df = pd.DataFrame(confidence_results)

# ===============================
# åŸºæœ¬è¯„ä¼°ç»“æœ
# ===============================
labels_sorted = sorted(unique_labels)

if "UNCERTAIN" in y_pred:
    y_pred_filtered = [
        p if p != "UNCERTAIN" else y_test[i] for i, p in enumerate(y_pred)
    ]
    print(f"\nå‘ç° {y_pred.count('UNCERTAIN')} ä¸ªä½ç½®ä¿¡åº¦é¢„æµ‹")
else:
    y_pred_filtered = y_pred

cm = confusion_matrix(y_test, y_pred_filtered, labels=labels_sorted)
acc = accuracy_score(y_test, y_pred_filtered)

print("\n" + "=" * 70)
print("=== åŸºæœ¬åˆ†ç±»ç»“æœ ===")
print("=" * 70)
print(f"å‡†ç¡®ç‡: {acc:.3f}")
print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")

# è·å–è¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆåŒ…å«F1åˆ†æ•°ï¼‰
report_dict = classification_report(
    y_test, y_pred_filtered, target_names=labels_sorted, output_dict=True
)
print(
    f"\nè¯¦ç»†æŠ¥å‘Š:\n{classification_report(y_test, y_pred_filtered, target_names=labels_sorted)}"
)

# æå–å„ç±»åˆ«F1åˆ†æ•°
print("\n" + "=" * 70)
print("=== å„ç±»åˆ«F1åˆ†æ•° ===")
print("=" * 70)
print(f"{'ç±»åˆ«':<15} {'F1-Score':<12} {'Precision':<12} {'Recall':<12} {'Support'}")
print("-" * 70)
for label in labels_sorted:
    f1 = report_dict[label]["f1-score"]
    precision = report_dict[label]["precision"]
    recall = report_dict[label]["recall"]
    support = report_dict[label]["support"]
    print(f"{label:<15} {f1:<12.3f} {precision:<12.3f} {recall:<12.3f} {support}")

# å®å¹³å‡å’ŒåŠ æƒå¹³å‡
macro_f1 = report_dict["macro avg"]["f1-score"]
weighted_f1 = report_dict["weighted avg"]["f1-score"]
print("-" * 70)
print(f"{'å®å¹³å‡ (Macro)':<15} {macro_f1:<12.3f}")
print(f"{'åŠ æƒå¹³å‡ (Weighted)':<15} {weighted_f1:<12.3f}")

# ===============================
# ç½®ä¿¡åº¦ç»Ÿè®¡åˆ†æ
# ===============================
print("\n" + "=" * 70)
print("=== ç½®ä¿¡åº¦åˆ†æ ===")
print("=" * 70)

print(f"\næ•´ä½“ç½®ä¿¡åº¦ç»Ÿè®¡:")
print(f"  å¹³å‡ç½®ä¿¡åº¦: {confidence_df['confidence_score'].mean():.3f}")
print(f"  ç½®ä¿¡åº¦ä¸­ä½æ•°: {confidence_df['confidence_score'].median():.3f}")
print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {confidence_df['confidence_score'].std():.3f}")

print(f"\nDTWè·ç¦»ç»Ÿè®¡:")
print(f"  å¹³å‡DTWè·ç¦»: {confidence_df['avg_distance'].mean():.2f}")
print(
    f"  æœ€å°DTWè·ç¦»èŒƒå›´: {confidence_df['min_distance'].min():.2f} ~ {confidence_df['min_distance'].max():.2f}"
)

correct_conf = confidence_df[confidence_df["is_correct"]]["confidence_score"].mean()
incorrect_conf = confidence_df[~confidence_df["is_correct"]]["confidence_score"].mean()
print(f"\næ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯”:")
print(f"  æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {correct_conf:.3f}")
print(f"  é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {incorrect_conf:.3f}")
print(f"  å·®å¼‚: {correct_conf - incorrect_conf:.3f}")

correct_dist = confidence_df[confidence_df["is_correct"]]["avg_distance"].mean()
incorrect_dist = confidence_df[~confidence_df["is_correct"]]["avg_distance"].mean()
print(f"\næ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„DTWè·ç¦»å¯¹æ¯”:")
print(f"  æ­£ç¡®é¢„æµ‹å¹³å‡è·ç¦»: {correct_dist:.2f}")
print(f"  é”™è¯¯é¢„æµ‹å¹³å‡è·ç¦»: {incorrect_dist:.2f}")

# ä¿å­˜åˆ†æç»“æœ
output_file = "confidence_analysis_dtw_knn.csv"
confidence_df.to_csv(output_file, index=False)
print(f"\nğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜è‡³: {output_file}")

print("\n" + "=" * 70)
print("=== ğŸ’¡ DTW-KNN ç‰¹ç‚¹è¯´æ˜ ===")
print("=" * 70)
print(
    f"""
ä¼˜ç‚¹:
  1. æ— éœ€è®­ç»ƒï¼Œç›´æ¥åŸºäºæ ·æœ¬å¯¹æ¯”
  2. å¯¹æ—¶é—´æ‰­æ›²ä¸æ•æ„Ÿï¼Œé€‚åˆä¸åŒé€Ÿåº¦çš„åŠ¨ä½œ
  3. å¯è§£é‡Šæ€§å¼ºï¼ˆå¯æŸ¥çœ‹æœ€è¿‘é‚»æ ·æœ¬ï¼‰
  4. å¯¹å°æ ·æœ¬å‹å¥½

ç¼ºç‚¹:
  1. é¢„æµ‹é€Ÿåº¦è¾ƒæ…¢ï¼ˆéœ€è¦è®¡ç®—æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„DTWè·ç¦»ï¼‰
  2. å†…å­˜å ç”¨å¤§ï¼ˆéœ€è¦å­˜å‚¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬ï¼‰
  3. å¯¹ç‰¹å¾ç¼©æ”¾æ•æ„Ÿ

è°ƒä¼˜å»ºè®®:
  1. è°ƒæ•´Kå€¼ï¼ˆå½“å‰: {K_NEIGHBORS}ï¼‰
  2. è®¾ç½®DTWçª—å£å¤§å°é™åˆ¶ï¼ˆåŠ é€Ÿè®¡ç®—ï¼‰
  3. å°è¯•ä¸åŒç‰¹å¾ç»„åˆ
  4. è€ƒè™‘ä½¿ç”¨FastDTWåŠ é€Ÿ

æ¨¡å‹æ€§èƒ½æ€»ç»“:
  - æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.3f}
  - æµ‹è¯•é›†å®å¹³å‡F1: {macro_f1:.3f}"""
)

if PERFORM_CV and len(sequences) >= N_FOLDS:
    print(f"  - äº¤å‰éªŒè¯å‡†ç¡®ç‡: {mean_acc:.3f} Â± {std_acc:.3f}")
    print(f"  - äº¤å‰éªŒè¯å®F1: {mean_f1:.3f} Â± {std_f1:.3f}")

print("=" * 70)
