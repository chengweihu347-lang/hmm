import os
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    f1_score,
)
from hmmlearn.hmm import GMMHMM
import warnings

warnings.filterwarnings("ignore")

# ===============================
# å‚æ•°è®¾ç½®
# ===============================
# æ³¨æ„: ä¸ºäº†åœ¨Canvasç¯å¢ƒä¸­æ›´ç¨³å®šè¿è¡Œï¼Œé€šå¸¸æ¨èä½¿ç”¨ç®€å•çš„ç›¸å¯¹è·¯å¾„æˆ–ç›´æ¥çš„æ–‡ä»¶åã€‚
# è¿™é‡Œä¿ç•™ç”¨æˆ·æä¾›çš„os.path.join(os.path.dirname(__file__)...)ç»“æ„ã€‚
DATA_FOLDER = "demobb"  # ç®€åŒ–è·¯å¾„
LABEL_FILE = "labels.csv"  # ç®€åŒ–è·¯å¾„

N_COMPONENTS = 3
N_MIXTURES = 3
MAX_ITER = 100
COV_TYPE = "diag"
RANDOM_STATE = 42
SCALE_FEATURES = True
NAN_STRATEGY = "interpolate"
MIN_VALID_FRAMES = 10

# ===============================
# ç‰¹å¾é…ç½®
# ===============================
USE_ANGLE = True  # æ˜¯å¦ä½¿ç”¨è§’åº¦
USE_VELOCITY = False  # æ˜¯å¦ä½¿ç”¨è§’é€Ÿåº¦
USE_ACCELERATION = False  # æ˜¯å¦ä½¿ç”¨è§’åŠ é€Ÿåº¦

# ===============================
# è¯„ä¼°ä¸ç­–ç•¥
# ===============================
# åºåˆ—é•¿åº¦å½’ä¸€åŒ–ï¼šTrue=ä½¿ç”¨å¹³å‡ä¼¼ç„¶ï¼ŒFalse=ä½¿ç”¨æ€»ä¼¼ç„¶
USE_NORMALIZED_LIKELIHOOD = True  # æ¨èTrueï¼Œè§£å†³é•¿åº¦ä¸ç­‰é—®é¢˜

# äº¤å‰éªŒè¯è®¾ç½®
USE_CROSS_VALIDATION = True  # æ˜¯å¦è¿›è¡Œäº¤å‰éªŒè¯
N_FOLDS = 5  # äº¤å‰éªŒè¯æŠ˜æ•°

# ç½®ä¿¡åº¦é˜ˆå€¼è®¾ç½®
CONFIDENCE_THRESHOLD_LOW = 0.3
CONFIDENCE_THRESHOLD_HIGH = 0.7

# é¢„æµ‹ç­–ç•¥é€‰æ‹©: 'standard', 'confidence_weighted', 'margin_based'
PREDICTION_STRATEGY = "standard"

np.random.seed(RANDOM_STATE)


# ===============================
# ç‰¹å¾æå–å‡½æ•°
# ===============================
def compute_derivatives(angles):
    """
    è®¡ç®—è§’åº¦çš„ä¸€é˜¶å¯¼æ•°(è§’é€Ÿåº¦)å’ŒäºŒé˜¶å¯¼æ•°(è§’åŠ é€Ÿåº¦)
    """
    # ä½¿ç”¨numpyçš„gradientè®¡ç®—å¯¼æ•°ï¼Œæ›´ç¨³å®š
    velocity = np.gradient(angles, axis=0)
    acceleration = np.gradient(velocity, axis=0)

    return velocity, acceleration


def extract_features(seq):
    """
    æ ¹æ®é…ç½®æå–ç‰¹å¾ï¼šè§’åº¦ã€è§’é€Ÿåº¦ã€è§’åŠ é€Ÿåº¦
    """
    features_list = []

    # 1. åŸå§‹è§’åº¦
    if USE_ANGLE:
        features_list.append(seq)

    # 2. è§’é€Ÿåº¦ï¼ˆä¸€é˜¶å¯¼æ•°ï¼‰
    if USE_VELOCITY:
        velocity, _ = compute_derivatives(seq)
        features_list.append(velocity)

    # 3. è§’åŠ é€Ÿåº¦ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
    if USE_ACCELERATION:
        _, acceleration = compute_derivatives(seq)
        features_list.append(acceleration)

    # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    if len(features_list) == 0:
        raise ValueError("è‡³å°‘éœ€è¦é€‰æ‹©ä¸€ç§ç‰¹å¾ç±»å‹ï¼")

    features = np.hstack(features_list)

    return features


# ===============================
# æ•°æ®åŠ è½½å‡½æ•°
# ===============================
def load_json_angles(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    frames = data["frames"]

    seq = []
    for f in frames:
        angles = [
            f["joints"]["shoulder_angle"],
            f["joints"]["elbow_angle"],
            f["joints"]["hip_angle"],
            f["joints"]["knee_angle"],
            f["joints"]["ankle_angle"],
        ]
        seq.append(angles)

    seq = np.array(seq, dtype=float)

    has_nan = np.isnan(seq).any()
    if has_nan:
        if NAN_STRATEGY == "interpolate":
            seq = interpolate_nan(seq)
        elif NAN_STRATEGY == "drop":
            seq = drop_nan_frames(seq)

    # æå–ç‰¹å¾ï¼ˆåŒ…å«è§’åŠ é€Ÿåº¦ï¼‰
    features = extract_features(seq)

    return features


def interpolate_nan(seq):
    df = pd.DataFrame(seq)
    df = df.interpolate(method="linear", limit_direction="both", axis=0)
    df = df.fillna(df.mean())
    df = df.fillna(0)
    return df.values


def drop_nan_frames(seq):
    mask = ~np.isnan(seq).any(axis=1)
    return seq[mask]


def is_valid_sequence(seq):
    if len(seq) < MIN_VALID_FRAMES:
        return False
    if np.isnan(seq).all():
        return False
    # HMMæ¨¡å‹å¯¹inf/nanå€¼æ•æ„Ÿï¼Œç¡®ä¿åºåˆ—ä¸å«è¿™äº›å€¼
    if not np.isfinite(seq).all():
        return False
    return True


# ===============================
# æ”¹è¿›çš„ä¼¼ç„¶è®¡ç®—ï¼ˆå¤„ç†åºåˆ—é•¿åº¦ä¸ç­‰ï¼‰
# ===============================
def compute_normalized_likelihood(model, seq, use_normalized=True):
    """
    è®¡ç®—å½’ä¸€åŒ–çš„ä¼¼ç„¶å€¼ï¼Œè§£å†³åºåˆ—é•¿åº¦ä¸ç­‰çš„é—®é¢˜
    """
    try:
        raw_log_likelihood = model.score(seq)
    except Exception as e:
        # print(f"  [Warning] model.score() failed: {e}. Returning -inf.")
        return -np.inf

    if use_normalized:
        # ä½¿ç”¨å¹³å‡ä¼¼ç„¶ï¼ˆé™¤ä»¥åºåˆ—é•¿åº¦ï¼‰
        if len(seq) == 0:
            return -np.inf
        normalized_likelihood = raw_log_likelihood / len(seq)
        return normalized_likelihood
    else:
        # ä½¿ç”¨åŸå§‹æ€»ä¼¼ç„¶
        return raw_log_likelihood


# ===============================
# ç½®ä¿¡åº¦è®¡ç®—å‡½æ•°
# ===============================
def calculate_confidence_metrics(log_likelihoods):
    """
    è®¡ç®—å¤šç§ç½®ä¿¡åº¦æŒ‡æ ‡
    """
    # è¿‡æ»¤æ‰ -inf çš„ä¼¼ç„¶å€¼ï¼Œä½†ä¿ç•™å¯¹åº”çš„æ ‡ç­¾
    filtered_scores = {
        k: v for k, v in log_likelihoods.items() if v > -np.inf and np.isfinite(v)
    }

    if not filtered_scores:
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥çš„æƒ…å†µ
        all_labels = list(log_likelihoods.keys())
        default_prob = 1.0 / len(all_labels) if all_labels else 0
        return {
            "predicted_label": "UNKNOWN",
            "max_probability": default_prob,
            "margin": 0.0,
            "entropy": np.log(len(all_labels)) if all_labels else 0,
            "normalized_entropy": 1.0,
            "confidence_score": 0.0,
            "all_probabilities": {label: default_prob for label in all_labels},
            "raw_scores": log_likelihoods,
        }

    sorted_scores = sorted(filtered_scores.items(), key=lambda x: x[1], reverse=True)
    best_label, best_score = sorted_scores[0]
    second_best_score = (
        sorted_scores[1][1] if len(sorted_scores) > 1 else best_score - 100
    )  # å¦‚æœåªæœ‰ä¸€ä¸ªæœ‰æ•ˆæ¨¡å‹ï¼Œç»™ä¸€ä¸ªå¤§å·®è·

    # 1. æ ‡å‡†åŒ–æ¦‚ç‡ï¼ˆSoftmaxï¼‰
    scores = np.array(list(filtered_scores.values()))

    # é˜²å¾¡-infï¼Œä½†ç”±äºå‰é¢è¿‡æ»¤äº†ï¼Œè¿™é‡Œä¸»è¦å¤„ç†æ•°å€¼ç¨³å®šæ€§
    if not scores.size:
        # åº”è¯¥ä¸ä¼šå‘ç”Ÿï¼Œä½†ä½œä¸ºé˜²å¾¡æ€§ç¼–ç¨‹
        best_label = "UNKNOWN"
        scores = np.array([0])
        max_probability = 0.0
    else:
        # å¯¹æ•°-å’Œ-æŒ‡æ•°æŠ€å·§ (LogSumExp Trick) ç¡®ä¿æ•°å€¼ç¨³å®š
        max_score = np.max(scores)
        with np.errstate(over="ignore", under="ignore"):
            exp_scores = np.exp(scores - max_score)

        sum_exp_scores = np.sum(exp_scores)

        if sum_exp_scores == 0 or not np.isfinite(sum_exp_scores):
            probabilities = np.zeros_like(scores)
            probabilities[np.argmax(scores)] = 1.0
        else:
            probabilities = exp_scores / sum_exp_scores

        max_probability = np.max(probabilities)

    # é‡æ–°æ˜ å°„å›æ‰€æœ‰æ ‡ç­¾
    all_probabilities = {label: 0.0 for label in log_likelihoods.keys()}
    for label, prob in zip(filtered_scores.keys(), probabilities):
        all_probabilities[label] = prob

    # 2. Margin
    margin = best_score - second_best_score

    # 3. Entropy
    probs = np.array(list(all_probabilities.values()))  # ä½¿ç”¨æ‰€æœ‰æ ‡ç­¾çš„æ¦‚ç‡
    probabilities_safe = probs + 1e-10
    entropy = -np.sum(probs * np.log(probabilities_safe))
    max_entropy = np.log(len(all_probabilities))
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0

    # 4. ç»¼åˆç½®ä¿¡åº¦åˆ†æ•°
    # ç½®ä¿¡åº¦é«˜ = æœ€å¤§æ¦‚ç‡é«˜ * ç†µä½ (ä½ä¸ç¡®å®šæ€§)
    confidence_score = max_probability * (1 - normalized_entropy)

    return {
        "predicted_label": best_label,
        "max_probability": max_probability,
        "margin": margin,
        "entropy": entropy,
        "normalized_entropy": normalized_entropy,
        "confidence_score": confidence_score,
        "all_probabilities": all_probabilities,
        "raw_scores": log_likelihoods,
    }


def predict_with_strategy(seq, models, strategy="standard"):
    """
    æ ¹æ®ä¸åŒç­–ç•¥è¿›è¡Œé¢„æµ‹ï¼ˆä½¿ç”¨å½’ä¸€åŒ–ä¼¼ç„¶ï¼‰
    """
    # ä½¿ç”¨å½’ä¸€åŒ–ä¼¼ç„¶è®¡ç®—
    logL = {
        label: compute_normalized_likelihood(
            models[label], seq, USE_NORMALIZED_LIKELIHOOD
        )
        for label in models
    }
    confidence_metrics = calculate_confidence_metrics(logL)

    if strategy == "standard":
        pred_label = confidence_metrics["predicted_label"]
    elif strategy == "confidence_weighted":
        if confidence_metrics["confidence_score"] < CONFIDENCE_THRESHOLD_LOW:
            pred_label = "UNCERTAIN"
        else:
            pred_label = confidence_metrics["predicted_label"]
    elif strategy == "margin_based":
        # Marginé˜ˆå€¼éœ€è¦æ ¹æ®ä¼¼ç„¶åˆ†æ•°èŒƒå›´è°ƒæ•´ï¼Œ5.0æ˜¯ä¸€ä¸ªç»éªŒå€¼
        if confidence_metrics["margin"] < 5.0:
            pred_label = "UNCERTAIN"
        else:
            pred_label = confidence_metrics["predicted_label"]
    else:
        pred_label = confidence_metrics["predicted_label"]

    return pred_label, confidence_metrics


# ===============================
# äº¤å‰éªŒè¯å‡½æ•°
# ===============================
def perform_cross_validation(sequences, labels, n_folds=5):
    """
    æ‰§è¡Œäº¤å‰éªŒè¯ï¼Œè®¡ç®—å¹³å‡å‡†ç¡®ç‡å’Œæ ‡å‡†å·®
    """
    print(f"\n{'='*70}")
    print(f"æ‰§è¡Œ {n_folds} æŠ˜äº¤å‰éªŒè¯...")
    print(f"{'='*70}")

    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)
    cv_accuracies = []
    cv_f1_scores = []
    fold_num = 1

    # ç¡®ä¿ labels æ˜¯ numpy æ•°ç»„
    labels_array = np.array(labels)

    for train_idx, val_idx in skf.split(sequences, labels_array):
        print(f"\nç¬¬ {fold_num}/{n_folds} æŠ˜:")

        # åˆ’åˆ†æ•°æ®
        X_train_fold = [sequences[i] for i in train_idx]
        y_train_fold = labels_array[train_idx]
        X_val_fold = [sequences[i] for i in val_idx]
        y_val_fold = labels_array[val_idx]

        # ç‰¹å¾æ ‡å‡†åŒ– (åœ¨foldå†…éƒ¨fitï¼Œé˜²æ­¢æ³„éœ²)
        if SCALE_FEATURES:
            scaler = StandardScaler()
            all_train_frames = np.vstack(X_train_fold)
            scaler.fit(all_train_frames)
            X_train_fold = [scaler.transform(x) for x in X_train_fold]
            X_val_fold = [scaler.transform(x) for x in X_val_fold]

        # è®­ç»ƒæ¨¡å‹
        models_fold = {}
        unique_labels = sorted(set(y_train_fold))

        for label in unique_labels:
            label_seqs = [
                X_train_fold[i]
                for i in range(len(X_train_fold))
                if y_train_fold[i] == label
            ]
            if not label_seqs:
                print(
                    f"  [Warning] CV Fold {fold_num}: ç±»åˆ« {label} æ²¡æœ‰è®­ç»ƒæ ·æœ¬ï¼Œè·³è¿‡ã€‚"
                )
                continue

            # è¿‡æ»¤æ‰ä¸åˆæ³•çš„åºåˆ—ï¼Œé˜²æ­¢æ¨¡å‹è®­ç»ƒå¤±è´¥
            valid_seqs = [seq for seq in label_seqs if is_valid_sequence(seq)]
            if not valid_seqs:
                print(
                    f"  [Warning] CV Fold {fold_num}: ç±»åˆ« {label} æœ‰æ•ˆåºåˆ—å¤ªå°‘ï¼Œè·³è¿‡ã€‚"
                )
                continue

            X_concat = np.vstack(valid_seqs)
            lengths = [len(x) for x in valid_seqs]

            # å¼ºåˆ¶è¦æ±‚HMMçš„ç‰¹å¾ç»´åº¦å’Œæ•°æ®ç»´åº¦ä¸€è‡´
            if X_concat.shape[1] != feature_dim:
                print(
                    f"  [Warning] CV Fold {fold_num}: ç»´åº¦ä¸åŒ¹é… {X_concat.shape[1]} != {feature_dim}ï¼Œè·³è¿‡ã€‚"
                )
                continue

            model = GMMHMM(
                n_components=N_COMPONENTS,
                n_mix=N_MIXTURES,
                covariance_type=COV_TYPE,
                n_iter=MAX_ITER,
                random_state=RANDOM_STATE,
                verbose=False,
            )
            try:
                model.fit(X_concat, lengths)
                models_fold[label] = model
            except Exception as e:
                print(f"  [Error] CV Fold {fold_num}: è®­ç»ƒ {label} æ¨¡å‹å¤±è´¥: {e}")

        # é¢„æµ‹
        y_pred_fold = []
        for seq in X_val_fold:
            # ç¡®ä¿æ¨¡å‹è¢«è®­ç»ƒäº†
            if not models_fold:
                pred_label = y_val_fold[0]  # å›é€€åˆ°ç¬¬ä¸€ä¸ªæ ‡ç­¾
            else:
                pred_label, _ = predict_with_strategy(seq, models_fold, "standard")
            y_pred_fold.append(pred_label)

        # ç§»é™¤ 'UNKNOWN' æˆ– 'N/A' ç­‰æ— æ•ˆé¢„æµ‹ï¼Œä»…è¯„ä¼°æœ‰æ•ˆæ ‡ç­¾
        valid_labels_in_fold = list(models_fold.keys())

        # ç­›é€‰å‡ºåœ¨æ¨¡å‹ä¸­å­˜åœ¨çš„æ ‡ç­¾ï¼Œå¦åˆ™classification_reportä¼šæŠ¥é”™
        y_true_valid = [
            y_val_fold[i]
            for i, pred in enumerate(y_pred_fold)
            if pred in valid_labels_in_fold
        ]
        y_pred_valid = [pred for pred in y_pred_fold if pred in valid_labels_in_fold]

        if len(y_true_valid) == 0:
            fold_acc = 0.0
            fold_f1 = 0.0
        else:
            fold_acc = accuracy_score(y_true_valid, y_pred_valid)
            fold_f1 = f1_score(
                y_true_valid, y_pred_valid, average="macro", zero_division=0
            )

        cv_accuracies.append(fold_acc)
        cv_f1_scores.append(fold_f1)

        print(f"  æœ‰æ•ˆæ ·æœ¬å‡†ç¡®ç‡: {fold_acc:.4f}, Macro F1åˆ†æ•°: {fold_f1:.4f}")
        fold_num += 1

    return cv_accuracies, cv_f1_scores


# ===============================
# åŠ è½½æ•°æ®
# ===============================
print("=" * 70)
print("æ¨¡å‹: HMM-GMM (Hidden Markov Model)")
print("å‚æ•°:")
print(f"  - éšçŠ¶æ€æ•° (N_COMPONENTS): {N_COMPONENTS}")
print(f"  - æ··åˆé«˜æ–¯æ•° (N_MIXTURES): {N_MIXTURES}")
print(f"  - ä¼¼ç„¶å½’ä¸€åŒ– (USE_NORMALIZED_LIKELIHOOD): {USE_NORMALIZED_LIKELIHOOD}")
print("-" * 70)
print("ç‰¹å¾é…ç½®:")
print(f"  - ä½¿ç”¨è§’åº¦: {USE_ANGLE}")
print(f"  - ä½¿ç”¨è§’é€Ÿåº¦: {USE_VELOCITY}")
print(f"  - ä½¿ç”¨è§’åŠ é€Ÿåº¦: {USE_ACCELERATION}")

# è®¡ç®—ç‰¹å¾ç»´åº¦
feature_dim = 0
if USE_ANGLE:
    feature_dim += 5
if USE_VELOCITY:
    feature_dim += 5
if USE_ACCELERATION:
    feature_dim += 5
print(f"  - æ€»ç‰¹å¾ç»´åº¦: {feature_dim}")
print("=" * 70)

if not os.path.exists(LABEL_FILE):
    raise FileNotFoundError(f"æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {LABEL_FILE}")

labels_df = pd.read_csv(LABEL_FILE)
print(f"\nLoaded labels.csv: {len(labels_df)} entries")

sequences, labels, video_ids = [], [], []
skipped_count = 0

for _, row in labels_df.iterrows():
    json_path = os.path.join(DATA_FOLDER, row["video_id"])
    if os.path.exists(json_path):
        try:
            seq = load_json_angles(json_path)
            if is_valid_sequence(seq):
                sequences.append(seq)
                labels.append(row["label"])
                video_ids.append(row["video_id"])
            else:
                skipped_count += 1
        except Exception as e:
            # print(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶å¤±è´¥: {row['video_id']}, é”™è¯¯: {e}")
            skipped_count += 1
    else:
        skipped_count += 1

print(f"\nå·²æˆåŠŸåŠ è½½ {len(sequences)} ä¸ªæ ·æœ¬")
print(f"è·³è¿‡/å¤±è´¥: {skipped_count} ä¸ªæ ·æœ¬")

label_counts = pd.Series(labels).value_counts()
print(f"\nå„ç±»åˆ«æ ·æœ¬æ•°:\n{label_counts}")

# æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡ŒCV
if len(sequences) < N_FOLDS * 2 or len(label_counts) < 2:
    print(
        f"\næ ·æœ¬æ€»æ•° ({len(sequences)}) æˆ–ç±»åˆ«æ•° ({len(label_counts)}) è¿‡å°‘ï¼Œè·³è¿‡äº¤å‰éªŒè¯ã€‚"
    )
    USE_CROSS_VALIDATION = False

# ===============================
# äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
# ===============================
if USE_CROSS_VALIDATION:
    cv_accuracies, cv_f1_scores = perform_cross_validation(sequences, labels, N_FOLDS)

    print(f"\n{'='*70}")
    print("=== äº¤å‰éªŒè¯ç»“æœæ±‡æ€» ===")
    print(f"{'='*70}")

    if cv_accuracies:
        print(f"\nå‡†ç¡®ç‡:")
        print(f"  å„æŠ˜: {[f'{acc:.4f}' for acc in cv_accuracies]}")
        print(
            f"  å¹³å‡: {np.mean(cv_accuracies):.4f} Â± {np.std(cv_accuracies):.4f} (æ ‡å‡†å·®)"
        )
        print(f"  æœ€å°: {np.min(cv_accuracies):.4f}")
        print(f"  æœ€å¤§: {np.max(cv_accuracies):.4f}")

        print(f"\nMacro F1åˆ†æ•°:")
        print(f"  å„æŠ˜: {[f'{f1:.4f}' for f1 in cv_f1_scores]}")
        print(
            f"  å¹³å‡: {np.mean(cv_f1_scores):.4f} Â± {np.std(cv_f1_scores):.4f} (æ ‡å‡†å·®)"
        )
        print(f"  æœ€å°: {np.min(cv_f1_scores):.4f}")
        print(f"  æœ€å¤§: {np.max(cv_f1_scores):.4f}")

        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
        cv_results = pd.DataFrame(
            {
                "fold": range(1, N_FOLDS + 1),
                "accuracy": cv_accuracies,
                "f1_score": cv_f1_scores,
            }
        )
        cv_results.to_csv("cross_validation_results.csv", index=False)
        print(f"\nğŸ’¾ äº¤å‰éªŒè¯ç»“æœå·²ä¿å­˜è‡³: cross_validation_results.csv")
    else:
        print("âš ï¸ äº¤å‰éªŒè¯æœªèƒ½è®¡ç®—å‡ºæœ‰æ•ˆç»“æœã€‚")

# ===============================
# åˆ’åˆ†æ•°æ®é›† (ç”¨äºæœ€ç»ˆæµ‹è¯•)
# ===============================
if len(sequences) < 2:
    print("\n[Fatal Error] æ ·æœ¬æ•°ä¸è¶³ä»¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚é€€å‡ºã€‚")
    exit()

X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
    sequences,
    labels,
    video_ids,
    test_size=0.3,
    random_state=RANDOM_STATE,
    stratify=labels,
)
print(f"\n{'='*70}")
print(f"æœ€ç»ˆæµ‹è¯•é›†åˆ’åˆ†: è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")

# ===============================
# ç‰¹å¾ç¼©æ”¾
# ===============================
if SCALE_FEATURES:
    scaler = StandardScaler()
    all_train_frames = np.vstack(X_train)
    scaler.fit(all_train_frames)
    X_train = [scaler.transform(x) for x in X_train]

    # åœ¨æµ‹è¯•é›†ç¼©æ”¾æ—¶ï¼ŒåŒæ—¶è®°å½•åºåˆ—é•¿åº¦
    X_test_scaled = []
    test_lengths = []
    for x in X_test:
        X_test_scaled.append(scaler.transform(x))
        test_lengths.append(len(x))
    X_test = X_test_scaled

    print("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ (åŸºäºè®­ç»ƒé›†)")
else:
    test_lengths = [len(x) for x in X_test]

# ===============================
# è®­ç»ƒæ¨¡å‹ (åŸºäºå®Œæ•´è®­ç»ƒé›†)
# ===============================
models = {}
unique_labels = sorted(set(y_train))

for label in unique_labels:
    label_seqs = [X_train[i] for i in range(len(X_train)) if y_train[i] == label]

    if not label_seqs:
        continue

    valid_seqs = [seq for seq in label_seqs if is_valid_sequence(seq)]
    if not valid_seqs:
        continue

    X_concat = np.vstack(valid_seqs)
    lengths = [len(x) for x in valid_seqs]

    # å†æ¬¡æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
    if X_concat.shape[1] != feature_dim:
        print(
            f"  [Warning] è®­ç»ƒé›†ç»´åº¦ä¸åŒ¹é… {X_concat.shape[1]} != {feature_dim}ï¼Œè·³è¿‡ {label} æ¨¡å‹è®­ç»ƒã€‚"
        )
        continue

    # print(f"\næ­£åœ¨è®­ç»ƒ HMM æ¨¡å‹: {label} ({len(valid_seqs)} æ¡åºåˆ—)...")
    model = GMMHMM(
        n_components=N_COMPONENTS,
        n_mix=N_MIXTURES,
        covariance_type=COV_TYPE,
        n_iter=MAX_ITER,
        random_state=RANDOM_STATE,
        verbose=False,
    )
    try:
        model.fit(X_concat, lengths)
        models[label] = model
        # print(f"âœ… å®Œæˆ {label} æ¨¡å‹è®­ç»ƒ")
    except Exception as e:
        print(f"  [Error] è®­ç»ƒ {label} æ¨¡å‹å¤±è´¥: {e}")

if not models:
    print("\n[Fatal Error] æ²¡æœ‰ä¸€ä¸ªæ¨¡å‹è¢«æˆåŠŸè®­ç»ƒã€‚é€€å‡ºã€‚")
    exit()

# ===============================
# é¢„æµ‹ä¸ç½®ä¿¡åº¦åˆ†æ
# ===============================
print(f"\nä½¿ç”¨é¢„æµ‹ç­–ç•¥: {PREDICTION_STRATEGY}")
y_pred = []
confidence_results = []

for i, seq in enumerate(X_test):
    # å¦‚æœåºåˆ—ä¸åˆæ³•ï¼Œåˆ™è·³è¿‡
    if not is_valid_sequence(seq):
        pred_label = "UNKNOWN"
    else:
        pred_label, conf_metrics = predict_with_strategy(
            seq, models, PREDICTION_STRATEGY
        )

    # ä»…åœ¨é¢„æµ‹æˆåŠŸæ—¶æ·»åŠ ç½®ä¿¡åº¦æŒ‡æ ‡
    if pred_label != "UNKNOWN":
        confidence_results.append(
            {
                "video_id": ids_test[i],
                "true_label": y_test[i],
                "predicted_label": pred_label,
                "sequence_length": test_lengths[i],
                "confidence_score": conf_metrics["confidence_score"],
                "max_probability": conf_metrics["max_probability"],
                "margin": conf_metrics["margin"],
                "entropy": conf_metrics["normalized_entropy"],
                "is_correct": pred_label == y_test[i],
                "all_probs": conf_metrics["all_probabilities"],
            }
        )
    else:
        # æ— æ³•é¢„æµ‹çš„æ ·æœ¬ä¹Ÿè®°å½•ä¸‹æ¥
        confidence_results.append(
            {
                "video_id": ids_test[i],
                "true_label": y_test[i],
                "predicted_label": "FAILED_TO_PREDICT",
                "sequence_length": test_lengths[i],
                "confidence_score": 0.0,
                "max_probability": 0.0,
                "margin": -np.inf,
                "entropy": 1.0,
                "is_correct": False,
                "all_probs": {},
            }
        )

confidence_df = pd.DataFrame(confidence_results)

# è¿‡æ»¤æ‰æ— æ³•é¢„æµ‹çš„æ ·æœ¬
valid_predictions_df = confidence_df[
    confidence_df["predicted_label"] != "FAILED_TO_PREDICT"
].copy()

# ===============================
# åŸºæœ¬è¯„ä¼°ç»“æœ (åŸºäºæµ‹è¯•é›†)
# ===============================
labels_sorted = sorted(models.keys())  # åªè¯„ä¼°è®­ç»ƒè¿‡çš„æ ‡ç­¾

y_test_filtered = valid_predictions_df["true_label"].tolist()
y_pred_filtered = valid_predictions_df["predicted_label"].tolist()

if "UNCERTAIN" in y_pred_filtered:
    uncertain_count = y_pred_filtered.count("UNCERTAIN")
    print(f"\nå‘ç° {uncertain_count} ä¸ªä½ç½®ä¿¡åº¦é¢„æµ‹ (ç­–ç•¥: {PREDICTION_STRATEGY})")
    # è¯„ä¼°æ—¶ï¼Œå°† 'UNCERTAIN' è§†ä¸ºé”™è¯¯é¢„æµ‹ï¼ˆå¦‚æœçœŸå®æ ‡ç­¾ä¸æ˜¯ 'UNCERTAIN'ï¼‰
    y_pred_eval = [
        (
            p
            if p != "UNCERTAIN"
            else (
                y_test_filtered[i]
                if y_test_filtered[i] == "UNCERTAIN"
                else "UNCERTAIN_AS_ERROR"
            )
        )
        for i, p in enumerate(y_pred_filtered)
    ]
else:
    y_pred_eval = y_pred_filtered
    uncertain_count = 0


# ç¡®ä¿è¯„ä¼°ä½¿ç”¨çš„æ ‡ç­¾é›†åŒ…å«æ‰€æœ‰å¯èƒ½ç»“æœ (åŒ…æ‹¬å¯èƒ½å¼•å…¥çš„ 'UNCERTAIN_AS_ERROR')
all_eval_labels = sorted(list(set(y_test_filtered) | set(y_pred_eval)))
if "UNCERTAIN_AS_ERROR" in all_eval_labels:
    all_eval_labels.remove("UNCERTAIN_AS_ERROR")

cm = confusion_matrix(y_test_filtered, y_pred_eval, labels=all_eval_labels)
acc = accuracy_score(y_test_filtered, y_pred_eval)

report_dict = classification_report(
    y_test_filtered,
    y_pred_eval,
    target_names=all_eval_labels,
    output_dict=True,
    zero_division=0,
)
f1_macro = report_dict["macro avg"]["f1-score"]
f1_weighted = report_dict["weighted avg"]["f1-score"]

print("\n" + "=" * 70)
print("=== åŸºæœ¬åˆ†ç±»ç»“æœ (æµ‹è¯•é›†) ===")
print("=" * 70)
print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(valid_predictions_df)}")
print(f"å‡†ç¡®ç‡: {acc:.3f} (åŒ…å« 'UNCERTAIN' é”™è¯¯)")
print(f"Macro F1åˆ†æ•°: {f1_macro:.3f}")

print(f"\næ··æ·†çŸ©é˜µ (è¡Œ=çœŸå®, åˆ—=é¢„æµ‹):\n{cm}")
print(f"æ ‡ç­¾: {all_eval_labels}")
print(
    f"\nè¯¦ç»†æŠ¥å‘Š:\n{classification_report(y_test_filtered, y_pred_eval, target_names=all_eval_labels, zero_division=0)}"
)


# ===============================
# ç½®ä¿¡åº¦ç»Ÿè®¡åˆ†æ
# ===============================
print("\n" + "=" * 70)
print("=== ç½®ä¿¡åº¦åˆ†æ (æµ‹è¯•é›†) ===")
print("=" * 70)

if valid_predictions_df.empty:
    print("æ²¡æœ‰æœ‰æ•ˆé¢„æµ‹æ ·æœ¬è¿›è¡Œç½®ä¿¡åº¦åˆ†æã€‚")
else:
    print(f"\næ•´ä½“ç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {valid_predictions_df['confidence_score'].mean():.3f}")
    print(f"  ç½®ä¿¡åº¦ä¸­ä½æ•°: {valid_predictions_df['confidence_score'].median():.3f}")
    print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {valid_predictions_df['confidence_score'].std():.3f}")

    # æ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯”
    print(f"\næ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦å¯¹æ¯” (ä¸å«UNCRETRAIN):")
    correct_conf = valid_predictions_df[valid_predictions_df["is_correct"]][
        "confidence_score"
    ].mean()
    incorrect_conf = valid_predictions_df[~valid_predictions_df["is_correct"]][
        "confidence_score"
    ].mean()
    print(f"  æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {correct_conf:.3f}")
    print(f"  é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {incorrect_conf:.3f}")
    print(f"  å·®å¼‚ (è¶Šæ­£è¶Šå¥½): {correct_conf - incorrect_conf:.3f}")


# ===============================
# æ–°å¢ï¼šé•¿åºåˆ— vs. çŸ­åºåˆ—æ€§èƒ½æ¯”è¾ƒ
# ===============================
print("\n" + "=" * 70)
print("=== åºåˆ—é•¿åº¦æ€§èƒ½æ¯”è¾ƒ ===")
print("=" * 70)

if valid_predictions_df.empty:
    print("æ²¡æœ‰æœ‰æ•ˆé¢„æµ‹æ ·æœ¬è¿›è¡Œé•¿åº¦æ¯”è¾ƒã€‚")
else:
    # è®¡ç®—æµ‹è¯•é›†æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬çš„å¹³å‡é•¿åº¦
    avg_length = valid_predictions_df["sequence_length"].mean()
    print(f"æµ‹è¯•é›†å¹³å‡åºåˆ—é•¿åº¦: {avg_length:.1f} å¸§")

    # åˆ’åˆ†é•¿åºåˆ—å’ŒçŸ­åºåˆ—
    long_sequences = valid_predictions_df[
        valid_predictions_df["sequence_length"] > avg_length
    ]
    short_sequences = valid_predictions_df[
        valid_predictions_df["sequence_length"] <= avg_length
    ]

    print("\n--- çŸ­åºåˆ— (é•¿åº¦ <= å¹³å‡) ---")
    if not short_sequences.empty:
        short_acc = short_sequences["is_correct"].mean()
        short_conf = short_sequences["confidence_score"].mean()
        print(f"  æ ·æœ¬æ•°: {len(short_sequences)}")
        print(f"  å‡†ç¡®ç‡: {short_acc:.3f}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {short_conf:.3f}")
    else:
        print("  æ— æ ·æœ¬ã€‚")

    print("\n--- é•¿åºåˆ— (é•¿åº¦ > å¹³å‡) ---")
    if not long_sequences.empty:
        long_acc = long_sequences["is_correct"].mean()
        long_conf = long_sequences["confidence_score"].mean()
        print(f"  æ ·æœ¬æ•°: {len(long_sequences)}")
        print(f"  å‡†ç¡®ç‡: {long_acc:.3f}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {long_conf:.3f}")
    else:
        print("  æ— æ ·æœ¬ã€‚")

    # æ€»ç»“å¯¹æ¯”
    if not long_sequences.empty and not short_sequences.empty:
        acc_diff = long_acc - short_acc
        conf_diff = long_conf - short_conf
        print("\n--- ç»“è®º ---")
        print(f"é•¿åºåˆ— vs. çŸ­åºåˆ— å‡†ç¡®ç‡å·®å¼‚ (é•¿-çŸ­): {acc_diff:+.3f}")
        if abs(acc_diff) > 0.05:
            print("âš ï¸ å‡†ç¡®ç‡å·®å¼‚è¾ƒå¤§ï¼Œæ¨¡å‹å¯¹åºåˆ—é•¿åº¦å­˜åœ¨åå¥½ã€‚")

        print(f"é•¿åºåˆ— vs. çŸ­åºåˆ— å¹³å‡ç½®ä¿¡åº¦å·®å¼‚ (é•¿-çŸ­): {conf_diff:+.3f}")
        if conf_diff < -0.1:
            print("âš ï¸ é•¿åºåˆ—çš„ç½®ä¿¡åº¦æ˜¾è‘—åä½ï¼Œå¯èƒ½æ„å‘³ç€é•¿åº¦å½’ä¸€åŒ–ä¸å¤Ÿå……åˆ†ã€‚")
        elif conf_diff > 0.1:
            print("âš ï¸ é•¿åºåˆ—çš„ç½®ä¿¡åº¦æ˜¾è‘—åé«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦è‡ªä¿¡ã€‚")

    # ç¡®ä¿ä¿å­˜çš„DFåŒ…å«sequence_length
    confidence_df.to_csv("confidence_analysis_full.csv", index=False)
    print(f"\nğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜è‡³: confidence_analysis_full.csv")


# ===============================
# ä½/é«˜ç½®ä¿¡åº¦æ ·æœ¬åˆ†æ (åŸºäºæœ‰æ•ˆé¢„æµ‹)
# ===============================
if not valid_predictions_df.empty:

    # ä½ç½®ä¿¡åº¦æ ·æœ¬
    low_conf_samples = valid_predictions_df[
        valid_predictions_df["confidence_score"] < CONFIDENCE_THRESHOLD_LOW
    ].sort_values("confidence_score")

    print("\n" + "=" * 70)
    print("=== ä½ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆéœ€äººå·¥å¤æ ¸ï¼‰===")
    print("=" * 70)
    print(
        f"\nå‘ç° {len(low_conf_samples)} ä¸ªä½ç½®ä¿¡åº¦æ ·æœ¬ (é˜ˆå€¼ < {CONFIDENCE_THRESHOLD_LOW}):"
    )

    if len(low_conf_samples) > 0:
        print(
            f"\n{'è§†é¢‘ID':<30} {'çœŸå®':<10} {'é¢„æµ‹':<10} {'é•¿åº¦':<8} {'ç½®ä¿¡åº¦':<10} {'æ­£ç¡®?'}"
        )
        print("-" * 80)
        for _, row in low_conf_samples.head(10).iterrows():
            check = "âœ“" if row["is_correct"] else "âœ—"
            print(
                f"{row['video_id']:<30} {row['true_label']:<10} {row['predicted_label']:<10} "
                f"{row['sequence_length']:<8} {row['confidence_score']:<10.3f} {check}"
            )

        low_conf_file = "low_confidence_samples.csv"
        low_conf_samples.to_csv(low_conf_file, index=False)
        print(f"\nğŸ’¾ ä½ç½®ä¿¡åº¦æ ·æœ¬å·²ä¿å­˜è‡³: {low_conf_file}")

    # é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬
    high_conf_errors = valid_predictions_df[
        (~valid_predictions_df["is_correct"])
        & (valid_predictions_df["confidence_score"] > CONFIDENCE_THRESHOLD_HIGH)
    ].sort_values("confidence_score", ascending=False)

    print("\n" + "=" * 70)
    print("=== é«˜ç½®ä¿¡åº¦ä½†é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ï¼ˆé‡ç‚¹ï¼ï¼‰===")
    print("=" * 70)

    if len(high_conf_errors) > 0:
        print(
            f"\nå‘ç° {len(high_conf_errors)} ä¸ªé«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬ (ç½®ä¿¡åº¦ > {CONFIDENCE_THRESHOLD_HIGH}):"
        )
        print("è¿™äº›æ ·æœ¬å¯èƒ½å­˜åœ¨æ ‡ç­¾é”™è¯¯æˆ–ç‰¹å¾æå–é—®é¢˜ï¼\n")

        print(f"{'è§†é¢‘ID':<30} {'çœŸå®':<10} {'é¢„æµ‹':<10} {'é•¿åº¦':<8} {'ç½®ä¿¡åº¦':<10}")
        print("-" * 80)
        for _, row in high_conf_errors.head(10).iterrows():
            print(
                f"{row['video_id']:<30} {row['true_label']:<10} {row['predicted_label']:<10} "
                f"{row['sequence_length']:<8} {row['confidence_score']:<10.3f}"
            )

        high_conf_errors.to_csv("high_confidence_errors.csv", index=False)
        print(f"\nğŸ’¾ é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬å·²ä¿å­˜è‡³: high_confidence_errors.csv")
    else:
        print("\næœªå‘ç°é«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬ã€‚")

print("\n" + "=" * 70)
print("åˆ†æå®Œæˆï¼")
print("=" * 70)
