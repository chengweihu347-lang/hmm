"""
HMMè¶…å‚æ•°ç½‘æ ¼æœç´¢è„šæœ¬
ç”¨äºç³»ç»Ÿåœ°æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œæ‰¾åˆ°æœ€ä½³precision
"""
# å‚æ•°æœç´¢èŒƒå›´
# N_COMPONENTS_RANGE = [2, 3, 4, 5]  # éšçŠ¶æ€æ•°
# N_MIXTURES_RANGE = [2, 3, 4]  # é«˜æ–¯æ··åˆæ•°
# COV_TYPES = ["diag", "full"]  # åæ–¹å·®ç±»å‹
# MAX_ITER_RANGE = [100, 200]  # æœ€å¤§è¿­ä»£æ¬¡æ•°
# æ”¹è¿›æ„è§ï¼š ç‰¹å¾ä¼˜åŒ–ï¼Œè§’åº¦çš„å¹³æ–¹        æ•°æ®å¹³è¡¡å¤„ç†ï¼Œç±»åˆ«å¹³è¡¡ï¼

import os
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from hmmlearn.hmm import GMMHMM
from itertools import product
import time

# ===============================
# åŸºç¡€é…ç½®
# ===============================
DATA_FOLDER = os.path.join(os.path.dirname(__file__), "demobb")
LABEL_FILE = os.path.join(os.path.dirname(__file__), "labels.csv")
RANDOM_STATE = 42
NAN_STRATEGY = "interpolate"
MIN_VALID_FRAMES = 10

np.random.seed(RANDOM_STATE)

# ===============================
# è¶…å‚æ•°æœç´¢ç©ºé—´
# ===============================
PARAM_GRID = {
    'n_components': [3, 4, 5, 6],           # éšçŠ¶æ€æ•°
    'n_mixtures': [1, 2, 3],                # GMMæ··åˆæ•°
    'covariance_type': ['diag', 'full'],    # åæ–¹å·®ç±»å‹
    'max_iter': [100],                       # EMè¿­ä»£æ¬¡æ•°ï¼ˆå¯å›ºå®šï¼‰
}

# å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆç”¨äºå¿«é€ŸéªŒè¯ï¼‰
PARAM_GRID_FAST = {
    'n_components': [3, 4, 5],
    'n_mixtures': [1, 2],
    'covariance_type': ['diag'],
    'max_iter': [50],
}

# é€‰æ‹©æœç´¢æ¨¡å¼
USE_FAST_MODE = False  # True=å¿«é€Ÿæ¨¡å¼, False=å®Œæ•´æœç´¢

# ===============================
# æ•°æ®åŠ è½½å‡½æ•°ï¼ˆåŒåŸè„šæœ¬ï¼‰
# ===============================
def interpolate_nan(seq):
    df = pd.DataFrame(seq)
    df = df.interpolate(method='linear', limit_direction='both', axis=0)
    df = df.fillna(df.mean())
    df = df.fillna(0)
    return df.values

def drop_nan_frames(seq):
    mask = ~np.isnan(seq).any(axis=1)
    return seq[mask]

def load_json_angles(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    frames = data["frames"]

    seq = []
    for f in frames:
        angles = [
            f["joints"]["shoulder_angle"],
            f["joints"]["elbow_angle"],
            f["joints"]["hip_angle"],
            f["joints"]["knee_angle"],
            f["joints"]["ankle_angle"],
        ]
        seq.append(angles)

    seq = np.array(seq, dtype=float)
    #     # æ·»åŠ è§’åº¦çš„å¹³æ–¹ä½œä¸ºç‰¹å¾
    # angles_squared = angles ** 2
    # seq.append(np.concatenate([angles, angles_squared]))
    # return np.array(seq)
    # è§’åº¦æƒé‡å¢åŠ 
    # if label == "jg":
    #     weights = [1.0, 1.0, 1.0, 1.0, 2.0]  # å¯¹jgç±»åˆ«ï¼Œankleè§’åº¦æƒé‡åŠ å€
    # else:
    #     weights = [1.0, 1.0, 1.0, 1.0, 1.0]  # å…¶ä»–ç±»åˆ«ä¿æŒåŸå§‹æƒé‡
    # # åº”ç”¨æƒé‡
    # seq = seq * weights

    if np.isnan(seq).any():
        if NAN_STRATEGY == "interpolate":
            seq = interpolate_nan(seq)
        elif NAN_STRATEGY == "drop":
            seq = drop_nan_frames(seq)

    return seq

def is_valid_sequence(seq):
    if len(seq) < MIN_VALID_FRAMES:
        return False
    if np.isnan(seq).all():
        return False
    return True

# ===============================
# åŠ è½½æ•°æ®
# ===============================
print("åŠ è½½æ•°æ®...")
labels_df = pd.read_csv(LABEL_FILE)
sequences, labels = [], []

for _, row in labels_df.iterrows():
    json_path = os.path.join(DATA_FOLDER, row["video_id"])
    if os.path.exists(json_path):
        try:
            seq = load_json_angles(json_path)
            if is_valid_sequence(seq):
                sequences.append(seq)
                labels.append(row["label"])
        except Exception as e:
            continue

print(f"æˆåŠŸåŠ è½½ {len(sequences)} ä¸ªæ ·æœ¬")

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    sequences, labels, test_size=0.3, random_state=RANDOM_STATE, stratify=labels
)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
all_train_frames = np.vstack(X_train)
scaler.fit(all_train_frames)
X_train = [scaler.transform(x) for x in X_train]
X_test = [scaler.transform(x) for x in X_test]

print(f"è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")

# ===============================
# è®­ç»ƒä¸è¯„ä¼°å‡½æ•°
# ===============================
def train_and_evaluate(n_components, n_mixtures, covariance_type, max_iter):
    """
    è®­ç»ƒHMMæ¨¡å‹å¹¶è¿”å›è¯„ä¼°æŒ‡æ ‡
    """
    models = {}
    unique_labels = sorted(set(y_train))
    
    try:
        # è®­ç»ƒæ¯ç±»æ¨¡å‹
        for label in unique_labels:
            label_seqs = [X_train[i] for i in range(len(X_train)) if y_train[i] == label]
            X_concat = np.vstack(label_seqs)
            lengths = [len(x) for x in label_seqs]
            
            model = GMMHMM(
                n_components=n_components,
                n_mix=n_mixtures,
                covariance_type=covariance_type,
                n_iter=max_iter,
                random_state=RANDOM_STATE,
                verbose=False,
            )
            model.fit(X_concat, lengths)
            models[label] = model
        
        # é¢„æµ‹
        y_pred = []
        for seq in X_test:
            logL = {label: models[label].score(seq) for label in models}
            pred_label = max(logL, key=logL.get)
            y_pred.append(pred_label)
        
        # è®¡ç®—æŒ‡æ ‡ï¼ˆmacroå¹³å‡ï¼‰
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
        
        return accuracy, precision, recall, f1, True
    
    except Exception as e:
        print(f"  è®­ç»ƒå¤±è´¥: {e}")
        return 0, 0, 0, 0, False

# ===============================
# ç½‘æ ¼æœç´¢
# ===============================
param_grid = PARAM_GRID_FAST if USE_FAST_MODE else PARAM_GRID

# ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
param_combinations = list(product(
    param_grid['n_components'],
    param_grid['n_mixtures'],
    param_grid['covariance_type'],
    param_grid['max_iter']
))

print(f"\nå¼€å§‹ç½‘æ ¼æœç´¢ ({'å¿«é€Ÿæ¨¡å¼' if USE_FAST_MODE else 'å®Œæ•´æ¨¡å¼'})")
print(f"æ€»å…± {len(param_combinations)} ç§å‚æ•°ç»„åˆ\n")
print("="*80)

results = []
best_precision = 0
best_params = None

for idx, (n_comp, n_mix, cov_type, max_iter) in enumerate(param_combinations, 1):
    print(f"\n[{idx}/{len(param_combinations)}] æµ‹è¯•å‚æ•°:")
    print(f"  n_components={n_comp}, n_mixtures={n_mix}, cov_type={cov_type}, max_iter={max_iter}")
    
    start_time = time.time()
    acc, prec, rec, f1, success = train_and_evaluate(n_comp, n_mix, cov_type, max_iter)
    elapsed = time.time() - start_time
    
    if success:
        print(f"  âœ… Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f} (è€—æ—¶: {elapsed:.1f}s)")
        
        results.append({
            'n_components': n_comp,
            'n_mixtures': n_mix,
            'covariance_type': cov_type,
            'max_iter': max_iter,
            'accuracy': acc,
            'precision': prec,
            'recall': rec,
            'f1_score': f1,
            'time': elapsed
        })
        
        # æ›´æ–°æœ€ä½³ç»“æœ
        if prec > best_precision:
            best_precision = prec
            best_params = (n_comp, n_mix, cov_type, max_iter)
            print(f"  ğŸŒŸ æ–°çš„æœ€ä½³Precision!")

# ===============================
# è¾“å‡ºç»“æœ
# ===============================
print("\n" + "="*80)
print("=== è°ƒå‚ç»“æœæ±‡æ€» ===")
print("="*80)

# è½¬æ¢ä¸ºDataFrame
results_df = pd.DataFrame(results)

# æŒ‰precisionæ’åº
results_df = results_df.sort_values('precision', ascending=False)

print("\nå‰10ä¸ªæœ€ä½³é…ç½®ï¼ˆæŒ‰Precisionæ’åºï¼‰:")
print(results_df.head(10).to_string(index=False))

print(f"\næœ€ä½³å‚æ•°ç»„åˆ:")
print(f"  n_components: {best_params[0]}")
print(f"  n_mixtures: {best_params[1]}")
print(f"  covariance_type: {best_params[2]}")
print(f"  max_iter: {best_params[3]}")
print(f"  â†’ æœ€ä½³Precision: {best_precision:.4f}")

# ä¿å­˜ç»“æœ
output_file = "hmm_tuning_results.csv"
results_df.to_csv(output_file, index=False)
print(f"\nå®Œæ•´ç»“æœå·²ä¿å­˜è‡³: {output_file}")

# ===============================
# å‚æ•°å½±å“åˆ†æ
# ===============================
print("\n" + "="*80)
print("=== å‚æ•°å½±å“åˆ†æ ===")
print("="*80)

if len(results_df) > 0:
    # æŒ‰å„ä¸ªå‚æ•°åˆ†ç»„ï¼ŒæŸ¥çœ‹å¹³å‡precision
    print("\nå„å‚æ•°å¯¹Precisionçš„å½±å“:")
    
    for param in ['n_components', 'n_mixtures', 'covariance_type']:
        print(f"\n{param}:")
        grouped = results_df.groupby(param)['precision'].agg(['mean', 'std', 'max'])
        print(grouped.round(4))

print("\n" + "="*80)
print("è°ƒå‚å»ºè®®:")
print("1. å¦‚æœæ‰€æœ‰é…ç½®precisionéƒ½å¾ˆä½(<0.5)ï¼Œè€ƒè™‘:")
print("   - æ£€æŸ¥æ•°æ®è´¨é‡å’Œæ ‡ç­¾æ˜¯å¦æ­£ç¡®")
print("   - å¢åŠ è®­ç»ƒæ ·æœ¬æ•°é‡")
print("   - å°è¯•ä¸åŒçš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•")
print("\n2. å¦‚æœæŸä¸ªå‚æ•°æ˜æ˜¾æ›´ä¼˜ï¼Œå›ºå®šè¯¥å‚æ•°åç»§ç»­ç»†åŒ–å…¶ä»–å‚æ•°")
print("\n3. å¦‚æœcovariance_type='full'æ•ˆæœå¥½ä½†è®­ç»ƒæ…¢ï¼Œè€ƒè™‘å¢åŠ æ ·æœ¬åå†ç”¨")
print("="*80)
